{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d10ef8e",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Harvard-CS1090/2026_CS1090B_public/blob/main/sec04/cs1090b_sec04_student.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e171b165",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\">\n",
    "\n",
    "# CS1090B Section 4: CNNs, Data Pipelines, and Transfer Learning\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Spring 2026**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, and Chris Gumb<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a509a",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this section, we explore Convolutional Neural Networks (CNNs), PyTorch data pipelines, and transfer learning.\n",
    "\n",
    "- **Part 1** reviews the building blocks of a CNN â€” convolutional layers, pooling, flatten, dense, dropout, and global average pooling â€” and builds a LeNet-style architecture from scratch.\n",
    "- **Part 2** applies CNNs to classify the Kannada-MNIST dataset. We compare a fully-connected baseline against a CNN, then use the CNN as a feature extractor for a Random Forest classifier.\n",
    "- **Part 3** introduces PyTorch data pipelines: `Dataset`, `DataLoader`, transforms, data augmentation, and train/validation splitting using the Horses-or-Humans dataset.\n",
    "- **Part 4** demonstrates transfer learning with a pretrained MobileNetV2 to classify horses vs. humans with minimal training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h0da4xovk7",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this section, you should be able to:\n",
    "\n",
    "### Conceptual Understanding\n",
    "1. Explain why CNNs are better suited for image data than fully-connected networks (spatial structure, weight sharing, local receptive fields).\n",
    "2. Describe the role of each CNN building block â€” convolution, pooling, flatten, dense, dropout, and global average pooling.\n",
    "3. Explain how a trained CNN can serve as a feature extractor for classical ML models.\n",
    "4. Articulate why transfer learning works: what general features pretrained models learn and how they transfer to new tasks.\n",
    "5. Distinguish between preprocessing transforms (deterministic, applied everywhere) and augmentation transforms (stochastic, training only).\n",
    "\n",
    "### Practical Skills (PyTorch)\n",
    "6. Build a CNN architecture from scratch using `nn.Module` and calculate feature map dimensions through the network.\n",
    "7. Train and evaluate binary classifiers using `BCEWithLogitsLoss` with early stopping.\n",
    "8. Use `torchvision.datasets.ImageFolder` to load image datasets organized by class directories.\n",
    "9. Construct PyTorch data pipelines with `Dataset`, `DataLoader`, and `transforms.Compose`.\n",
    "10. Apply data augmentation (flips, rotations, color jitter) to training data using torchvision transforms.\n",
    "11. Perform transfer learning: load a pretrained model, freeze the backbone, and train a new classifier head."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2abb04",
   "metadata": {},
   "source": [
    "## Setup: Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b4a158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment detection and setup\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import shutil\n",
    "import zipfile\n",
    "import urllib.request\n",
    "\n",
    "# --- 1. Download notebook_assets.zip (kmnist data, CNN features, figures) ---\n",
    "assets_zip_url = \"https://github.com/Harvard-CS1090/2026_CS1090B_public/raw/main/sec04/notebook_assets.zip\"\n",
    "assets_zip_name = \"notebook_assets.zip\"\n",
    "expected_dirs = [\"data\", \"fig\"]\n",
    "\n",
    "all_dirs_exist = all(os.path.isdir(d) for d in expected_dirs)\n",
    "\n",
    "if all_dirs_exist:\n",
    "    print(\"Required directories already exist. Skipping asset download.\")\n",
    "else:\n",
    "    print(f\"Downloading {assets_zip_name} from GitHub...\")\n",
    "    try:\n",
    "        if 'google.colab' in sys.modules:\n",
    "            subprocess.run(['wget', '-q', assets_zip_url], check=True)\n",
    "        else:\n",
    "            urllib.request.urlretrieve(assets_zip_url, assets_zip_name)\n",
    "        with zipfile.ZipFile(assets_zip_name, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(\".\")\n",
    "        os.remove(assets_zip_name)\n",
    "        if os.path.isdir('__MACOSX'):\n",
    "            shutil.rmtree('__MACOSX')\n",
    "        print(\"Asset download complete.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during asset setup: {e}\", file=sys.stderr)\n",
    "\n",
    "# --- 2. Download Horses-or-Humans images (for Parts 3 & 4) ---\n",
    "hoh_dir = \"data/horses_or_humans\"\n",
    "if os.path.isdir(hoh_dir):\n",
    "    print(\"Horses-or-Humans data already exists. Skipping download.\")\n",
    "else:\n",
    "    os.makedirs(hoh_dir, exist_ok=True)\n",
    "    base_url = \"https://storage.googleapis.com/learning-datasets\"\n",
    "    for split, fname in [(\"train\", \"horse-or-human.zip\"), (\"test\", \"validation-horse-or-human.zip\")]:\n",
    "        url = f\"{base_url}/{fname}\"\n",
    "        dest_zip = os.path.join(hoh_dir, fname)\n",
    "        dest_dir = os.path.join(hoh_dir, split)\n",
    "        print(f\"Downloading {fname}...\")\n",
    "        urllib.request.urlretrieve(url, dest_zip)\n",
    "        with zipfile.ZipFile(dest_zip, \"r\") as zf:\n",
    "            zf.extractall(dest_dir)\n",
    "        os.remove(dest_zip)\n",
    "    print(\"Horses-or-Humans download complete.\")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791c699e",
   "metadata": {},
   "source": [
    "## Part 1: CNN Building Blocks\n",
    "\n",
    "<img src=\"./fig/cnn1.png\" width=\"800\">\n",
    "\n",
    "The following is a review of the core layers used when building CNNs with PyTorch.\n",
    "A link to the official PyTorch documentation for each layer is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4a8825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import random as rn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset, random_split, Subset\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Device setup\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA device found, using CUDA.\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS device found, using MPS.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"No GPU found, using CPU.\")\n",
    "\n",
    "\n",
    "# Reproducibility\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "SEED = 109\n",
    "np.random.seed(SEED)\n",
    "rn.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28092e6",
   "metadata": {},
   "source": [
    "### 2D Convolutional Layers\n",
    "\n",
    "[**torch.nn.Conv2d**](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)`(in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True)`\n",
    "\n",
    "<img src=\"./fig/conv-many-filters.png\" width=\"800\">\n",
    "\n",
    "**Quick review:**\n",
    "\n",
    "A convolutional layer is composed of **filters**, which are composed of **kernels** which are themselves composed of **weights**. Each filter also has a bias term (often omitted in diagrams). We learn the weights and biases from our data. Each conv layer also has an associated **activation function** such as ReLU or sigmoid.\n",
    "\n",
    "The **number of filters** and the **height and width of the kernels** are set by the `out_channels` and `kernel_size` arguments respectively.\n",
    "\n",
    "The **depth of the filters is fixed** by the depth (i.e., `in_channels`) of the input to the conv layer.\n",
    "\n",
    "The output of the conv layer is a 3D tensor â€” a set of **feature maps**. Each feature map is the output of one filter convolving on the input. The height and width of the output is determined by the input size, `kernel_size`, `padding`, and `stride`. The depth of the output tensor (number of feature maps) equals the number of filters.\n",
    "\n",
    "**Important:** PyTorch uses **NCHW** format (batch, channels, height, width) by default.\n",
    "\n",
    "PyTorch also has [1D convolutional layers](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html) for time series and [3D convolutional layers](https://pytorch.org/docs/stable/generated/torch.nn.Conv3d.html) for video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947c8c65",
   "metadata": {},
   "source": [
    "### Pooling Layers\n",
    "\n",
    "[**torch.nn.MaxPool2d**](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)`(kernel_size, stride=None, padding=0)`\n",
    "\n",
    "<img src=\"./fig/maxpool.png\" width=\"800\">\n",
    "\n",
    "Pooling layers reduce spatial dimensions. A `MaxPool2d` with a 2Ã—2 kernel and stride 2 outputs feature maps that are half the size of the input. For each 2Ã—2 region, only the maximum value is kept.\n",
    "\n",
    "When `stride` is not specified, it defaults to `kernel_size`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757da8a7",
   "metadata": {},
   "source": [
    "### Flatten Layers\n",
    "\n",
    "[**torch.nn.Flatten**](https://docs.pytorch.org/docs/stable/generated/torch.flatten.html)`(start_dim=1)`\n",
    "\n",
    "`Flatten` has no learned parameters. It reshapes the multi-dimensional feature maps into a flat vector. It sits between the final convolutional/pooling output and the first fully connected layer. The default `start_dim=1` preserves the batch dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c942848",
   "metadata": {},
   "source": [
    "### Fully Connected Layers\n",
    "\n",
    "[**torch.nn.Linear**](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)`(in_features, out_features, bias=True)`\n",
    "\n",
    "<img src=\"./fig/dense.png\" width=\"800\">\n",
    "\n",
    "Most CNNs have one or more fully connected (linear) layers at the end, with the final layer referred to as the **output layer**. You specify `in_features` and `out_features` (the number of neurons).\n",
    "\n",
    "PyTorch's `nn.Linear` does **not** include an activation function â€” you apply activations separately (e.g., `nn.ReLU()` as the next layer).\n",
    "\n",
    "*The choice of activation for the output layer depends on the task:* linear for regression, sigmoid for binary classification, softmax (or no activation with `CrossEntropyLoss`) for multi-class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa1e1f4",
   "metadata": {},
   "source": [
    "### Dropout Layers\n",
    "\n",
    "[**torch.nn.Dropout**](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)`(p=0.5)`\n",
    "\n",
    "<img src=\"./fig/dropout.gif\" width=\"800\">\n",
    "                                     \n",
    "Dropout randomly sets a fraction p of input units to 0 during training, and scales the remaining activations by $1/(1-p)$ to maintain the expected output magnitude. At evaluation time, all units are active and no scaling is applied. Dropout helps prevent overfitting by limiting the model's ability to co-adapt neurons.  \n",
    "\n",
    "**Caution:** Dropout's behavior differs when applied after convolutional vs. dense layers! Standard `nn.Dropout` drops individual values; for conv layers, `nn.Dropout2d` drops entire channels.\n",
    "\n",
    "**Important:** In PyTorch, you must call `model.train()` before training and `model.eval()` before evaluation to toggle dropout (and batch norm) behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae42798d",
   "metadata": {},
   "source": [
    "### Global Average Pooling\n",
    "\n",
    "[**torch.nn.AdaptiveAvgPool2d**](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html)`(output_size)`\n",
    "\n",
    "Global Average Pooling (GAP) calculates the average value of each feature map, reducing the spatial dimensions to 1Ã—1. If a convolutional layer outputs feature maps of shape `(C, H, W)`, applying `AdaptiveAvgPool2d(1)` reduces this to `(C, 1, 1)`, which can be squeezed to a flat vector of length `C`.\n",
    "\n",
    "**Why use GAP?**\n",
    "\n",
    "- **Reduces overfitting:** Summarizing spatial information reduces total parameters compared to using `Flatten` + `Linear` directly after conv layers.\n",
    "- **Seamless transition to classification:** The output is a flat vector of length equal to the number of feature maps, which can be fed directly into a classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2960ba86",
   "metadata": {},
   "source": [
    "> **â“ Question 1: CNN Layer Concepts**\n",
    ">\n",
    "> 1. What determines the depth (number of channels) of a convolutional layer's output?\n",
    "> 2. Why might it make sense to think of dropout as a type of ensemble method?\n",
    "> 3. What is the advantage of Global Average Pooling over Flatten before the classifier?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d11bf74",
   "metadata": {},
   "source": [
    "### Exercise: Building LeNet from Scratch\n",
    "\n",
    "Now let's build a CNN model with a similar architecture to [**LeNet**](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf) from scratch using `nn.Module`.\n",
    "\n",
    "The architecture:\n",
    "- **Input:** 32Ã—32 grayscale images (1 channel)\n",
    "- **Conv1:** 6 filters, kernel 5Ã—5, stride 1, no padding â†’ ReLU\n",
    "- **Pool1:** MaxPool 2Ã—2, stride 2\n",
    "- **Conv2:** 16 filters, kernel 5Ã—5, stride 1, no padding â†’ ReLU\n",
    "- **Pool2:** MaxPool 2Ã—2, stride 2\n",
    "- **Conv3:** 120 filters, kernel 5Ã—5 â†’ ReLU\n",
    "- **Flatten**\n",
    "- **FC1:** 84 units â†’ ReLU\n",
    "- **Output:** 10 units (raw logits â€” use with `CrossEntropyLoss`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77c7180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a LeNet model from scratch using nn.Module\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ntu4wiocdmg",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_summary(model, input_size):\n",
    "    \"\"\"Print layer-by-layer output shapes and parameter counts.\"\"\"\n",
    "    headers = f\"{'Layer':<35} {'Output Shape':<20} {'Params':>10}\"\n",
    "    print(headers)\n",
    "    print(\"=\" * len(headers))\n",
    "\n",
    "    total_params = 0\n",
    "    def hook_fn(name):\n",
    "        def hook(module, input, output):\n",
    "            nonlocal total_params\n",
    "            params = sum(p.numel() for p in module.parameters(recurse=False))\n",
    "            total_params += params\n",
    "            shape = list(output.shape)\n",
    "            print(f\"{name:<35} {str(shape):<20} {params:>10,}\")\n",
    "        return hook\n",
    "\n",
    "    hooks = []\n",
    "    for name, layer in model.named_modules():\n",
    "        if not name or list(layer.children()):  # skip root and container modules\n",
    "            continue\n",
    "        hooks.append(layer.register_forward_hook(hook_fn(name)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model(torch.randn(*input_size))\n",
    "\n",
    "    for h in hooks:\n",
    "        h.remove()\n",
    "    print(\"=\" * len(headers))\n",
    "    print(f\"{'Total':<55} {total_params:>10,}\")\n",
    "\n",
    "\n",
    "model_summary(lenet, (1, 1, 32, 32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb93178c",
   "metadata": {},
   "source": [
    "> **â“ Question 2: LeNet Architecture**\n",
    ">\n",
    "> 1. After Conv1 (kernel 5Ã—5, no padding) and Pool1 (2Ã—2), what is the spatial size of the feature maps? Start from 32Ã—32 input.\n",
    "> 2. Why does Conv3 have output spatial size 1Ã—1?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c181b7a3",
   "metadata": {},
   "source": [
    "## Part 2: CNN Classification with KMNIST\n",
    "\n",
    "Now that we understand the building blocks â€” convolutional layers, pooling, flatten, dense, dropout, and GAP â€” let's put them to work on a real classification task. We'll see how a CNN compares to a fully-connected network, and explore a powerful idea: using the CNN's learned features as input to a classical ML model.\n",
    "\n",
    "### The Kannada MNIST Dataset\n",
    "\n",
    "![Kannada MNIST](https://www.googleapis.com/download/storage/v1/b/kaggle-user-content/o/inbox%2F3630446%2F1e01bcc28b5ccb7ad38a4ffefb13cde0%2Fwondu.png?generation=1603204077179447&alt=media)\n",
    "\n",
    "For this part, we will work with a modified version of the [Kannada MNIST dataset](https://arxiv.org/pdf/1908.01242.pdf), which is a large database of handwritten digits in a Dravidian language spoken in southern India.\n",
    "\n",
    "This dataset consists of 60,000 28Ã—28 grayscale images of ten digits, along with a test set of 10,000 images. For this section, we simplify the problem by using only the digits labeled `0` and `1` (which are visually similar), with 1,200 training samples and 2,000 test samples.\n",
    "\n",
    "To understand the dataset better, see this [article](https://medium.com/data-science/a-new-handwritten-digits-dataset-in-ml-town-kannada-mnist-69df0f2d1456) by Vinay Prabhu, the dataset curator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381f32da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load KMNIST data from CSV files\n",
    "\n",
    "df_train = pd.read_csv(\"data/kmnist_train.csv\")\n",
    "X_kmnist_train = df_train.drop(columns=\"output\").values\n",
    "y_kmnist_train = df_train[\"output\"].values\n",
    "\n",
    "df_test = pd.read_csv(\"data/kmnist_test.csv\")\n",
    "X_kmnist_test = df_test.drop(columns=\"output\").values\n",
    "y_kmnist_test = df_test[\"output\"].values\n",
    "\n",
    "# Reshape to NCHW format and normalize to [0, 1]\n",
    "X_kmnist_train = X_kmnist_train.reshape(-1, 1, 28, 28).astype(np.float32) / 255.0\n",
    "X_kmnist_test = X_kmnist_test.reshape(-1, 1, 28, 28).astype(np.float32) / 255.0\n",
    "\n",
    "print(\n",
    "    \"The shapes of the Kannada MNIST X and y datasets are:\\n\\n\"\n",
    "    f\"\\tX train\\t{X_kmnist_train.shape}\\n\\ty train\\t{y_kmnist_train.shape}\\n\"\n",
    "    f\"\\n\\tX test\\t{X_kmnist_test.shape}\\n\\ty test\\t{y_kmnist_test.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b6aee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample KMNIST training characters\n",
    "idx_list = [2, 1]\n",
    "fig, axes = plt.subplots(1, 2, figsize=(6, 3.5))\n",
    "\n",
    "plt.suptitle(\"Sample KMNIST training characters\", y=1, fontsize=18)\n",
    "\n",
    "for idx, ax in zip(idx_list, axes.flat):\n",
    "    ax.imshow(X_kmnist_train[idx, 0], cmap=\"gray\")  # index channel dim for imshow\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_title(f\"class label: {y_kmnist_train[idx]}\", fontsize=14)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8362bfb8",
   "metadata": {},
   "source": [
    "### Preparing Data for PyTorch\n",
    "\n",
    "We convert our NumPy arrays to PyTorch tensors, create `TensorDataset` objects, and wrap them in `DataLoader`s for batched training. We also split 20% of the training set for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028ea9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_t = torch.tensor(X_kmnist_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_kmnist_train, dtype=torch.float32)\n",
    "X_test_t = torch.tensor(X_kmnist_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_kmnist_test, dtype=torch.float32)\n",
    "\n",
    "# Train/validation split (80/20)\n",
    "n_val = int(0.2 * len(X_train_t))\n",
    "n_train = len(X_train_t) - n_val\n",
    "generator = torch.Generator().manual_seed(SEED)\n",
    "\n",
    "full_ds = TensorDataset(X_train_t, y_train_t)\n",
    "train_ds, val_ds = random_split(full_ds, [n_train, n_val], generator=generator)\n",
    "test_ds = TensorDataset(X_test_t, y_test_t)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"Train: {n_train}, Val: {n_val}, Test: {len(X_test_t)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96beb223",
   "metadata": {},
   "source": [
    "### Training Utilities\n",
    "\n",
    "We define reusable training, evaluation, and plotting functions for binary classification with `BCEWithLogitsLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8d10fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to halt training when validation loss stops improving.\"\"\"\n",
    "    def __init__(self, patience=10, min_delta=0.0, restore_best=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best = restore_best\n",
    "        self.best_loss = np.inf\n",
    "        self.best_state = None\n",
    "        self.counter = 0\n",
    "\n",
    "    def step(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if self.restore_best:\n",
    "                self.best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "    def restore(self, model):\n",
    "        if self.restore_best and self.best_state is not None:\n",
    "            model.load_state_dict(self.best_state)\n",
    "\n",
    "\n",
    "def train_binary(model, train_loader, val_loader, optimizer, epochs=100,\n",
    "                 early_stopping=None):\n",
    "    \"\"\"Train a binary classifier with BCEWithLogitsLoss.\"\"\"\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    history = {\"loss\": [], \"val_loss\": [], \"accuracy\": [], \"val_accuracy\": []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb).squeeze(1)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "            correct += ((torch.sigmoid(logits) > 0.5).float() == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "\n",
    "        model.eval()\n",
    "        val_loss_sum, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                logits = model(xb).squeeze(1)\n",
    "                loss = criterion(logits, yb)\n",
    "                val_loss_sum += loss.item() * xb.size(0)\n",
    "                val_correct += ((torch.sigmoid(logits) > 0.5).float() == yb).sum().item()\n",
    "                val_total += yb.size(0)\n",
    "\n",
    "        val_loss = val_loss_sum / val_total\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        history[\"loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"accuracy\"].append(train_acc)\n",
    "        history[\"val_accuracy\"].append(val_acc)\n",
    "\n",
    "        if early_stopping and early_stopping.step(val_loss, model):\n",
    "            break\n",
    "\n",
    "    if early_stopping:\n",
    "        early_stopping.restore(model)\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_binary(model, loader):\n",
    "    \"\"\"Evaluate a binary classifier; returns (loss, accuracy).\"\"\"\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    model.eval()\n",
    "    loss_sum, correct, total = 0.0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            logits = model(xb).squeeze(1)\n",
    "            loss_sum += criterion(logits, yb).item() * xb.size(0)\n",
    "            correct += ((torch.sigmoid(logits) > 0.5).float() == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "    return loss_sum / total, correct / total\n",
    "\n",
    "\n",
    "def plot_hist(history, title=None):\n",
    "    \"\"\"Plot training history (loss and accuracy).\"\"\"\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    axs[0].plot(history[\"loss\"], label=\"train\")\n",
    "    axs[0].plot(history[\"val_loss\"], label=\"val\")\n",
    "    best_epoch = np.argmin(history[\"val_loss\"])\n",
    "    axs[0].axvline(best_epoch, c=\"k\", ls=\":\", label=\"best val epoch\")\n",
    "    axs[0].set_xlabel(\"Epoch\")\n",
    "    axs[0].set_ylabel(\"BCE Loss\")\n",
    "    axs[0].legend()\n",
    "\n",
    "    axs[1].plot(history[\"accuracy\"], label=\"train\")\n",
    "    axs[1].plot(history[\"val_accuracy\"], label=\"val\")\n",
    "    axs[1].axvline(best_epoch, c=\"k\", ls=\":\", label=\"best val epoch\")\n",
    "    axs[1].set_xlabel(\"Epoch\")\n",
    "    axs[1].set_ylabel(\"Accuracy\")\n",
    "    axs[1].legend()\n",
    "\n",
    "    if title:\n",
    "        plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def print_test_results(model_name, test_results):\n",
    "    loss, acc = test_results\n",
    "    print(f\"{model_name} performance on test set:\\n\"\n",
    "          f\"\\tTest Loss: {loss:.4f}\\n\\tTest Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779380ff",
   "metadata": {},
   "source": [
    "### Fully-Connected Network (Baseline)\n",
    "\n",
    "First, let's build a fully-connected network similar to what we've seen before. The input images are flattened to 784-dimensional vectors. We use L2 regularization via `weight_decay` in the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5b5488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully-connected network with regularization\n",
    "torch.manual_seed(SEED)\n",
    "fcn_model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(784, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(100, 100),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.1),\n",
    "    nn.Linear(100, 1),\n",
    ").to(device)\n",
    "\n",
    "print(fcn_model)\n",
    "\n",
    "# Use weight_decay for L2 regularization \n",
    "optimizer = optim.Adam(fcn_model.parameters(), lr=0.001, weight_decay=0.003)\n",
    "early_stop = EarlyStopping(patience=10, restore_best=True)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "history_fcn = train_binary(fcn_model, train_loader, val_loader, optimizer,\n",
    "                           epochs=100, early_stopping=early_stop)\n",
    "\n",
    "plot_hist(history_fcn, \"FCN Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60596940",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_test_results(\"Fully-connected network\", evaluate_binary(fcn_model, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e942297",
   "metadata": {},
   "source": [
    "### Baseline CNN Classifier\n",
    "\n",
    "Now let's build a CNN that takes advantage of the spatial structure in the image data. We use three convolutional blocks (Conv â†’ ReLU â†’ MaxPool), followed by a flattened dense layer and a single logit output (used with `BCEWithLogitsLoss`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c244eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),  # 28x28 -> 28x28\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                              # -> 14x14\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),  # -> 14x14\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                              # -> 7x7\n",
    "            nn.Conv2d(32, 16, kernel_size=3, padding=1),  # -> 7x7\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                              # -> 3x3\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(16 * 3 * 3, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "baseline_cnn = BaselineCNN().to(device)\n",
    "print(baseline_cnn)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in baseline_cnn.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31rjytybl3m",
   "metadata": {},
   "source": [
    "ðŸ’¬ **Discuss:** Notice that `BaselineCNN` separates the network into `self.features` (conv blocks) and `self.classifier` (dense layers). Why might this separation be useful beyond just code organization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2303094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(baseline_cnn.parameters(), lr=0.001)\n",
    "early_stop = EarlyStopping(patience=10, restore_best=True)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "history_cnn = train_binary(baseline_cnn, train_loader, val_loader, optimizer,\n",
    "                           epochs=50, early_stopping=early_stop)\n",
    "\n",
    "plot_hist(history_cnn, \"Baseline CNN Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d29ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_test_results(\"Baseline CNN model\", evaluate_binary(baseline_cnn, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0gphv61ien3l",
   "metadata": {},
   "source": [
    "### CNN as Feature Extractor + Random Forest\n",
    "\n",
    "A trained CNN doesn't just classify - its convolutional layers learn **feature representations** that can be reused. The idea: pass images through the CNN backbone (`self.features`), discard the classifier head, and use the resulting feature vectors as input to a classical ML model like Random Forest.\n",
    "\n",
    "Our `BaselineCNN.features` outputs shape `(batch, 16, 3, 3)`. Flattened, that gives a **144-dimensional feature vector** per image â€” compared to the original 784 raw pixels. These 144 features encode high-level patterns (edges, curves, strokes) that the CNN learned during training.\n",
    "\n",
    "We pre-extracted these CNN features and saved them to CSV, so we can load them directly and train a Random Forest without needing to re-run the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d9kcfg2qg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-extracted CNN features\n",
    "df_feat_train = pd.read_csv(\"data/kmnist_cnn_features_train.csv\")\n",
    "df_feat_test = pd.read_csv(\"data/kmnist_cnn_features_test.csv\")\n",
    "\n",
    "X_train_cnn = df_feat_train.drop(columns=\"label\").values\n",
    "y_train_cnn = df_feat_train[\"label\"].values\n",
    "X_test_cnn = df_feat_test.drop(columns=\"label\").values\n",
    "y_test_cnn = df_feat_test[\"label\"].values\n",
    "\n",
    "print(f\"CNN features â€” Train: {X_train_cnn.shape}, Test: {X_test_cnn.shape}\")\n",
    "print(f\"  (144 features from BaselineCNN backbone vs. 784 raw pixels)\")\n",
    "\n",
    "# Train Random Forest on CNN features\n",
    "rf_cnn = RandomForestClassifier(n_estimators=200, random_state=SEED, n_jobs=-1)\n",
    "rf_cnn.fit(X_train_cnn, y_train_cnn)\n",
    "rf_cnn_acc = accuracy_score(y_test_cnn, rf_cnn.predict(X_test_cnn))\n",
    "print(f\"\\nRandom Forest (CNN features) Test Accuracy: {rf_cnn_acc:.4f}\")\n",
    "\n",
    "# Compare: Random Forest on raw pixels\n",
    "X_train_raw = X_kmnist_train.reshape(len(X_kmnist_train), -1)  # (1200, 784)\n",
    "X_test_raw = X_kmnist_test.reshape(len(X_kmnist_test), -1)     # (2000, 784)\n",
    "\n",
    "rf_raw = RandomForestClassifier(n_estimators=200, random_state=SEED, n_jobs=-1)\n",
    "rf_raw.fit(X_train_raw, y_kmnist_train)\n",
    "rf_raw_acc = accuracy_score(y_kmnist_test, rf_raw.predict(X_test_raw))\n",
    "print(f\"Random Forest (raw pixels)   Test Accuracy: {rf_raw_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cohyl2gxea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "print(\"=\" * 55)\n",
    "print(f\"{'Model':<35} {'Test Accuracy':>15}\")\n",
    "print(\"=\" * 55)\n",
    "print(f\"{'FCN (raw pixels)':<35} {'0.9265':>15}\")\n",
    "print(f\"{'Random Forest (raw pixels)':<35} {rf_raw_acc:>15.4f}\")\n",
    "print(f\"{'Baseline CNN (end-to-end)':<35} {'0.9455':>15}\")\n",
    "print(f\"{'Random Forest (CNN features)':<35} {rf_cnn_acc:>15.4f}\")\n",
    "print(\"=\" * 55)\n",
    "print(\"\\nCNN features give Random Forest a significant boost\")\n",
    "print(\"over raw pixels, approaching the end-to-end CNN.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0td8l0vb2da",
   "metadata": {},
   "source": [
    "ðŸ’¬ **Discuss:** The CNN uses ~33K parameters while the FCN uses ~90K, yet the CNN performs better. Why doesn't having more parameters automatically lead to better performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a488076b",
   "metadata": {},
   "source": [
    "> **â“ Question 3: FCN vs CNN vs Random Forest**\n",
    ">\n",
    "> 1. Why does the CNN outperform the fully-connected network on image data?\n",
    "> 2. Our `BaselineCNN` uses odd-sized kernels (3Ã—3) with `padding=1`. Why are odd-sized kernels preferred over even-sized kernels (e.g., 2Ã—2) in practice?\n",
    "> 3. Why does Random Forest perform much better on CNN-extracted features than on raw pixels?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165199b3",
   "metadata": {},
   "source": [
    "## Part 3: Data Pipelines in PyTorch\n",
    "\n",
    "In Part 2, we loaded KMNIST from pre-processed CSV files and manually converted NumPy arrays to tensors. That works for small, clean datasets â€” but real-world vision projects typically involve thousands of images stored as files on disk, often requiring resizing, normalization, and augmentation on the fly. PyTorch provides a flexible data loading system for exactly this purpose, built around two key classes:\n",
    "\n",
    "- [`torch.utils.data.Dataset`](https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) â€” represents a dataset (how to access individual samples)\n",
    "- [`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) â€” wraps a Dataset to provide batching, shuffling, and parallel loading\n",
    "\n",
    "We'll explore these using the **Horses-or-Humans** dataset â€” a collection of CGI-generated images for binary classification. We switch to this dataset because it uses the standard image-folder format typical of real vision tasks, giving us a chance to practice the full pipeline from raw images to training-ready batches. We'll use this same pipeline for transfer learning in Part 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa244fc9",
   "metadata": {},
   "source": [
    "### Loading a Dataset with ImageFolder\n",
    "\n",
    "** ðŸŽ or ðŸ§? **\n",
    "\n",
    "The [Horses or Humans](http://laurencemoroney.com/horses-or-humans-dataset) dataset contains CGI-rendered images of horses and humans. We downloaded and extracted the images in the setup cell above.\n",
    "\n",
    "`torchvision.datasets.ImageFolder` automatically loads images from a directory structure where each subdirectory name is a class label:\n",
    "```\n",
    "data/horses_or_humans/train/\n",
    "    horses/\n",
    "        horse01.png\n",
    "        horse02.png\n",
    "        ...\n",
    "    humans/\n",
    "        human01.png\n",
    "        human02.png\n",
    "        ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5a0fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Horses-or-Humans dataset using ImageFolder\n",
    "simple_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "hoh_train = datasets.ImageFolder(\"data/horses_or_humans/train\", transform=simple_transform)\n",
    "hoh_test = datasets.ImageFolder(\"data/horses_or_humans/test\", transform=simple_transform)\n",
    "\n",
    "print(f\"Training samples: {len(hoh_train)}\")\n",
    "print(f\"Test samples:     {len(hoh_test)}\")\n",
    "print(f\"Classes:          {hoh_train.classes}\")\n",
    "print(f\"Class-to-idx:     {hoh_train.class_to_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8699f1c",
   "metadata": {},
   "source": [
    "### Iterables and Iterators\n",
    "\n",
    "A PyTorch `Dataset` is a map-style dataset â€” it implements `__getitem__` and `__len__`, making it subscriptable (you can index into it with `dataset[i]`). The `DataLoader` wraps a Dataset to provide iteration, batching, and shuffling.\n",
    "\n",
    "Recall from Python: an *iterable* has an `__iter__` method that returns an *iterator*, and an *iterator* has a `__next__` method that returns the next element. A `DataLoader` is an iterable that produces batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c9efb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access a single sample\n",
    "image, label = hoh_train[0]\n",
    "print(f\"Single image: shape={image.shape}, dtype={image.dtype}\")\n",
    "print(f\"Label: {label} ({hoh_train.classes[label]})\")\n",
    "\n",
    "# Visualize\n",
    "plt.imshow(image.permute(1, 2, 0))  # CHW -> HWC for matplotlib\n",
    "plt.title(hoh_train.classes[label])\n",
    "plt.axis(\"off\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48ab28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a grid of training samples\n",
    "rows, cols = 3, 5\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(10, 6))\n",
    "for i, ax in enumerate(axes.ravel()):\n",
    "    img, label = hoh_train[i]\n",
    "    ax.imshow(img.permute(1, 2, 0))\n",
    "    ax.set_title(hoh_train.classes[label])\n",
    "    ax.axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4200f0",
   "metadata": {},
   "source": [
    "### Batching with DataLoader\n",
    "\n",
    "To get the benefits of **stochastic gradient descent** (SGD), we feed data in batches. The `DataLoader` handles this automatically:\n",
    "\n",
    "```python\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "for images, labels in loader:\n",
    "    # images: (32, C, H, W), labels: (32,)\n",
    "    ...\n",
    "```\n",
    "\n",
    "Each iteration yields a batch of tensors, ready for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ede6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dl = DataLoader(hoh_train, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# How many batches?\n",
    "print(f\"Dataset size: {len(hoh_train)}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Number of batches: {len(train_dl)}\")\n",
    "print(f\"  (last batch may be smaller: {len(hoh_train) % batch_size} samples)\")\n",
    "\n",
    "# Inspect one batch\n",
    "batch_images, batch_labels = next(iter(train_dl))\n",
    "print(f\"\\nBatch image shape: {batch_images.shape}\")\n",
    "print(f\"Batch label shape: {batch_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d4c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first image from each batch\n",
    "fig, axs = plt.subplots(4, 8, figsize=(9, 5))\n",
    "axs_flat = axs.ravel()\n",
    "for i, (img_batch, label_batch) in enumerate(train_dl):\n",
    "    if i >= len(axs_flat):\n",
    "        break\n",
    "    axs_flat[i].imshow(img_batch[0].permute(1, 2, 0))\n",
    "    axs_flat[i].set_title(f\"batch {i+1}\", fontsize=7)\n",
    "    axs_flat[i].axis(\"off\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1907a5bd",
   "metadata": {},
   "source": [
    "### Optimizing Data Loading\n",
    "\n",
    "`DataLoader` provides several options to speed up training:\n",
    "\n",
    "- **`shuffle=True`** - Randomly reorders samples each epoch. Essential for training to prevent the model from learning the order of samples.\n",
    "\n",
    "- **`num_workers=N`** - Uses N subprocesses to load data in parallel. While the GPU processes the current batch, workers prepare the next batches. This overlaps data loading with computation.\n",
    "\n",
    "- **`pin_memory=True`** - Allocates data in page-locked (pinned) memory, enabling faster CPUâ†’GPU transfer. Use this when training on GPU.\n",
    "\n",
    "```python\n",
    "# Optimized DataLoader for GPU training\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True,\n",
    "                    num_workers=4, pin_memory=True)\n",
    "```\n",
    "\n",
    "**Note:** On macOS, `num_workers > 0` can sometimes cause issues. If you encounter problems, set `num_workers=0` (the default)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a604e73",
   "metadata": {},
   "source": [
    "### Preprocessing with Transforms\n",
    "\n",
    "In PyTorch, preprocessing is done via `torchvision.transforms.Compose`, which chains transformations applied to each image when it's loaded. Common transforms:\n",
    "\n",
    "- `transforms.Resize((H, W))` â€” Resize to target size\n",
    "- `transforms.ToTensor()` â€” Convert PIL image to tensor, scale to [0, 1]\n",
    "- `transforms.Normalize(mean, std)` â€” Normalize channels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2d12b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing transform: resize, convert to tensor, and normalize\n",
    "H = W = 224\n",
    "\n",
    "preprocess_transform = transforms.Compose([\n",
    "    transforms.Resize((H, W)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),  # ImageNet normalization\n",
    "])\n",
    "\n",
    "# Recreate datasets with preprocessing\n",
    "hoh_train_pp = datasets.ImageFolder(\"data/horses_or_humans/train\", transform=preprocess_transform)\n",
    "hoh_test_pp = datasets.ImageFolder(\"data/horses_or_humans/test\", transform=preprocess_transform)\n",
    "\n",
    "# Check a sample\n",
    "img, label = hoh_train_pp[0]\n",
    "print(f\"Preprocessed image: shape={img.shape}, min={img.min():.2f}, max={img.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee53fad4",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "We almost always wish we had *more data*! Data augmentation creates variants of our original images by applying random transformations. A horse is still a horse whether flipped or slightly rotated.\n",
    "\n",
    "In PyTorch, augmentation transforms are added to `transforms.Compose` and applied on-the-fly during training. Each epoch sees different random variants of the same images.\n",
    "\n",
    "Common augmentation transforms:\n",
    "- `transforms.RandomHorizontalFlip()` â€” flip left/right with 50% probability\n",
    "- `transforms.RandomRotation(degrees)` â€” random rotation within Â±degrees\n",
    "- `transforms.ColorJitter(brightness, contrast)` â€” random color adjustments\n",
    "- `transforms.RandomResizedCrop(size)` â€” random crop and resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3454b04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define augmentation pipeline for training\n",
    "augment_transform = transforms.Compose([\n",
    "    transforms.Resize((H, W)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "hoh_train_aug = datasets.ImageFolder(\"data/horses_or_humans/train\", transform=augment_transform)\n",
    "\n",
    "# Display augmented samples from the same image\n",
    "fig, axs = plt.subplots(2, 4, figsize=(10, 5))\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "    std=[1/0.229, 1/0.224, 1/0.225]\n",
    ")\n",
    "for ax in axs.ravel():\n",
    "    img, label = hoh_train_aug[0]  # same index, different augmentation each time\n",
    "    img_display = inv_normalize(img).clamp(0, 1)\n",
    "    ax.imshow(img_display.permute(1, 2, 0))\n",
    "    ax.axis(\"off\")\n",
    "plt.suptitle(\"Same image with different random augmentations\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba4d31d",
   "metadata": {},
   "source": [
    "> **â“ Question 4: Data Pipelines**\n",
    ">\n",
    "> 1. What is the difference between preprocessing transforms and augmentation transforms? When should each be applied?\n",
    "> 2. Why do we apply augmentation only to the training set and not the test set?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d813c95",
   "metadata": {},
   "source": [
    "### Creating a Validation Split\n",
    "\n",
    "The Horses-or-Humans dataset comes with predefined train and test splits, but no validation set. We can create one using `torch.utils.data.random_split()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b907dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split training data: 80% train, 20% validation\n",
    "generator = torch.Generator().manual_seed(SEED)\n",
    "train_size = int(0.8 * len(hoh_train_aug))\n",
    "val_size = len(hoh_train_aug) - train_size\n",
    "\n",
    "hoh_train_split, hoh_val_split = random_split(hoh_train_aug, [train_size, val_size],\n",
    "                                               generator=generator)\n",
    "\n",
    "# Note: the val split inherits the augmentation transform.\n",
    "# For proper evaluation, we should use non-augmented transforms for validation.\n",
    "# One approach: create a wrapper that overrides the transform.\n",
    "hoh_val_proper = datasets.ImageFolder(\"data/horses_or_humans/train\", transform=preprocess_transform)\n",
    "hoh_val_proper = Subset(hoh_val_proper, hoh_val_split.indices)\n",
    "\n",
    "print(f\"Train split: {len(hoh_train_split)}\")\n",
    "print(f\"Val split:   {len(hoh_val_proper)}\")\n",
    "print(f\"Test:        {len(hoh_test_pp)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035ca57c",
   "metadata": {},
   "source": [
    "> **â“ Question 5: Validation Pipeline**\n",
    ">\n",
    "> 1. Why do we apply the non-augmented preprocessing transform to the validation set instead of the augmented transform?\n",
    "> 2. What could go wrong if we accidentally shuffled the test set during evaluation?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fc802c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders for use in Parts 3 & 4\n",
    "hoh_train_loader = DataLoader(hoh_train_split, batch_size=32, shuffle=True)\n",
    "hoh_val_loader = DataLoader(hoh_val_proper, batch_size=32, shuffle=False)\n",
    "hoh_test_loader = DataLoader(hoh_test_pp, batch_size=32, shuffle=False)\n",
    "\n",
    "batch = next(iter(hoh_train_loader))\n",
    "print(f\"Batch shape: images={batch[0].shape}, labels={batch[1].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d1869e",
   "metadata": {},
   "source": [
    "## Part 4: Transfer Learning\n",
    "\n",
    "In Part 2, we saw that CNN features transfer well â€” even a Random Forest trained on CNN-extracted features outperformed one trained on raw pixels. Transfer learning takes this idea further: instead of training a CNN from scratch on our small dataset, we start from a model already trained on millions of images and adapt it to our task. The data pipeline we built in Part 3 (with preprocessing, augmentation, and DataLoaders) is exactly what we need to feed images into a pretrained model.\n",
    "\n",
    "Instead of training from scratch, we:\n",
    "\n",
    "1. **Load a pretrained model** (e.g., MobileNetV2)\n",
    "2. **Freeze the base layers** â€” prevent their weights from updating\n",
    "3. **Replace the classifier head** â€” add new layers suited to our task\n",
    "4. **Train only the new head** on our data\n",
    "\n",
    "This works because early layers learn general features (edges, textures) that transfer across tasks, while later layers learn task-specific features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99be5d9f",
   "metadata": {},
   "source": [
    "### MobileNetV2\n",
    "\n",
    "[MobileNetV2](https://pytorch.org/vision/stable/models/mobilenetv2.html) is a lightweight architecture designed for mobile and edge devices. It uses depthwise separable convolutions to achieve good accuracy with far fewer parameters than models like ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14b0e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained MobileNetV2\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "\n",
    "base_model = mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "\n",
    "# Freeze all base model parameters\n",
    "for param in base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace the classifier head\n",
    "# MobileNetV2 original: classifier = Sequential(Dropout(0.2), Linear(1280, 1000))\n",
    "# We replace with our binary classification head\n",
    "base_model.classifier = nn.Sequential(\n",
    "    nn.Dropout(0.2),\n",
    "    nn.Linear(1280, 1024),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(1024, 1),\n",
    ")\n",
    "\n",
    "base_model = base_model.to(device)\n",
    "\n",
    "# Only the new classifier parameters are trainable\n",
    "trainable_params = sum(p.numel() for p in base_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in base_model.parameters())\n",
    "print(f\"Trainable params: {trainable_params:,} / {total_params:,} total\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6528acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function for transfer learning (binary classification)\n",
    "# Note: This is similar to train_binary() from Part 2, adapted for ImageFolder\n",
    "# datasets (which return integer labels requiring .float() conversion) and with\n",
    "# per-epoch printing to monitor transfer learning progress.\n",
    "def train_transfer(model, train_loader, val_loader, optimizer, epochs=10):\n",
    "    criterion = nn.BCEWithLogitsLoss()  # raw logits, no sigmoid in model\n",
    "    history = {\"loss\": [], \"val_loss\": [], \"accuracy\": [], \"val_accuracy\": []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.float().to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images).squeeze(1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_acc = correct / total\n",
    "\n",
    "        model.eval()\n",
    "        val_loss_sum, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.float().to(device)\n",
    "                outputs = model(images).squeeze(1)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss_sum += loss.item() * images.size(0)\n",
    "                preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                val_correct += (preds == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_loss = val_loss_sum / val_total\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        history[\"loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"accuracy\"].append(train_acc)\n",
    "        history[\"val_accuracy\"].append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} â€” \"\n",
    "              f\"loss: {train_loss:.4f}, acc: {train_acc:.4f} â€” \"\n",
    "              f\"val_loss: {val_loss:.4f}, val_acc: {val_acc:.4f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(base_model.classifier.parameters(), lr=0.0001)\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "history_transfer = train_transfer(base_model, hoh_train_loader, hoh_val_loader,\n",
    "                                  optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b8d9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "base_model.eval()\n",
    "test_loss, test_correct, test_total = 0.0, 0, 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in hoh_test_loader:\n",
    "        images, labels = images.to(device), labels.float().to(device)\n",
    "        outputs = base_model(images).squeeze(1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * images.size(0)\n",
    "        preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        test_correct += (preds == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "\n",
    "print(f\"Test Loss: {test_loss/test_total:.4f}\")\n",
    "print(f\"Test Accuracy: {test_correct/test_total:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47mi89cnl",
   "metadata": {},
   "source": [
    "ðŸ’¬ **Discuss:** We reached ~96% test accuracy in just 10 epochs with only ~800 training images. MobileNetV2 was originally trained on ImageNet - over 1.2 million images across 1,000 classes. What challenges would you face trying to reach similar accuracy on Horses-or-Humans without a pretrained model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31a00ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot transfer learning training curves\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "axs[0].plot(history_transfer[\"loss\"], label=\"train\")\n",
    "axs[0].plot(history_transfer[\"val_loss\"], label=\"val\")\n",
    "axs[0].set_xlabel(\"Epoch\")\n",
    "axs[0].set_ylabel(\"BCE Loss\")\n",
    "axs[0].set_title(\"Transfer Learning â€” Loss\")\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].plot(history_transfer[\"accuracy\"], label=\"train\")\n",
    "axs[1].plot(history_transfer[\"val_accuracy\"], label=\"val\")\n",
    "axs[1].set_xlabel(\"Epoch\")\n",
    "axs[1].set_ylabel(\"Accuracy\")\n",
    "axs[1].set_title(\"Transfer Learning â€” Accuracy\")\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7cce01",
   "metadata": {},
   "source": [
    "> **â“ Question 6: Transfer Learning**\n",
    ">\n",
    "> 1. Why do we freeze the base model's parameters? What would happen if we didn't?\n",
    "> 2. The pretrained model was trained on ImageNet (1000 classes, natural images). Why does it work well for horses vs. humans, which is a very different task?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
