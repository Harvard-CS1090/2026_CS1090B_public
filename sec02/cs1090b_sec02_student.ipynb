{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS1090B Introduction to Data Science\n",
    "\n",
    "## Section 2: FeedForward Neural Networks - Optimizers\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Spring 2026**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, and Chris Gumb<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment detection and setup\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "# Define the zip file URL and expected directories\n",
    "assets_zip_url = \"https://github.com/Harvard-CS1090/2026_CS1090B_public/raw/main/sec02/notebook_assets.zip\"\n",
    "\n",
    "assets_zip_name = \"notebook_assets.zip\"\n",
    "expected_dirs = [\"data\", \"fig\"]\n",
    "\n",
    "# Check if required directories already exist\n",
    "all_dirs_exist = all(os.path.isdir(d) for d in expected_dirs)\n",
    "\n",
    "if all_dirs_exist:\n",
    "    print(\"Required directories already exist. Skipping download.\")\n",
    "else:\n",
    "    print(f\"Downloading {assets_zip_name} from GitHub...\")\n",
    "    \n",
    "    # Use wget in Colab, or urllib for local\n",
    "    try:\n",
    "        if 'google.colab' in sys.modules:\n",
    "            subprocess.run(['wget', '-q', assets_zip_url], check=True)\n",
    "        else:\n",
    "            import urllib.request\n",
    "            urllib.request.urlretrieve(assets_zip_url, assets_zip_name)\n",
    "        print(f\"Downloaded {assets_zip_name}.\")\n",
    "        \n",
    "        # Unzip the file\n",
    "        import zipfile\n",
    "        with zipfile.ZipFile(assets_zip_name, 'r') as zip_ref:\n",
    "            zip_ref.extractall('.')\n",
    "        print(f\"Extracted {assets_zip_name}.\")\n",
    "        \n",
    "        # Clean up the zip file\n",
    "        os.remove(assets_zip_name)\n",
    "        print(f\"Removed {assets_zip_name}.\")\n",
    "        \n",
    "        # Remove __MACOSX folder if it exists\n",
    "        if os.path.isdir('__MACOSX'):\n",
    "            shutil.rmtree('__MACOSX')\n",
    "            print(\"Removed __MACOSX folder.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during setup: {e}\", file=sys.stderr)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rn\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility settings\n",
    "SEED = 109\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "rn.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Regression with Neural Networks\n",
    "\n",
    "Let's fit a difficult function where polynomial regression fails.\n",
    "\n",
    "The [dielectric function](https://en.wikipedia.org/wiki/Permittivity) of many optical materials depends on the frequency and is given by the Lorentz model as:\n",
    "\n",
    "$$ \\varepsilon(\\omega) = 1 - \\frac{\\omega_0^2}{\\omega_0^2-\\omega^2 +i\\omega\\Gamma},$$\n",
    "\n",
    "where $\\omega$ is the frequency, $\\omega_0$ is the resonance frequency of the bound electrons, and $\\Gamma$ is the electron damping.\n",
    "\n",
    "In many situations, we measure the real part of the dielectric function in the lab and then we fit these observations. Let's assume that we perform an experiment and the observations come from a Lorentz model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/lorentz_set.csv').sample(frac=1, random_state=SEED)  # shuffle DataFrame\n",
    "print(\"Data shape: \",df.shape)\n",
    "# Split train and test - very small dataset so we skip untouched test set. \n",
    "x_train, x_test, y_train, y_test = train_test_split(df.x, df.y, train_size=0.7, random_state=SEED)\n",
    "\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "print(f\"Test data shape: {x_test.shape}\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lorentz(df, test_idx, ax=None):\n",
    "    \"\"\"Plot the Lorentz data with train/test split.\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    test_mask = df.index.isin(test_idx)\n",
    "    ax.scatter(df.x[~test_mask], df.y[~test_mask], c='b', label='train data')\n",
    "    ax.scatter(df.x[test_mask], df.y[test_mask], c='orange', marker='^', label='test data')\n",
    "    ax.set_xlabel(r'$\\omega$')\n",
    "    ax.set_ylabel(r'$\\epsilon$')\n",
    "    ax.legend()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plot_lorentz(df, x_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression Baseline\n",
    "\n",
    "Let's first try fitting a high-degree polynomial to see why neural networks might be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a 25-degree polynomial function to the data\n",
    "polynomial_features = PolynomialFeatures(degree=25)\n",
    "x_poly_train = polynomial_features.fit_transform(x_train.values.reshape(-1, 1))\n",
    "x_poly_test = polynomial_features.transform(x_test.values.reshape(-1, 1))\n",
    "\n",
    "\n",
    "model_poly = LinearRegression()\n",
    "model_poly.fit(x_poly_train, y_train)\n",
    "y_poly_train = model_poly.predict(x_poly_train)\n",
    "y_poly_test = model_poly.predict(x_poly_test)\n",
    "\n",
    "mse_train_poly = mean_squared_error(y_train, y_poly_train)\n",
    "mse_test_poly = mean_squared_error(y_test, y_poly_test)\n",
    "print('MSE on training set:', mse_train_poly)\n",
    "print('MSE on testing set:', mse_test_poly)\n",
    "\n",
    "x_lin = np.linspace(x_train.min(), x_train.max(), 1000)\n",
    "x_lin_poly = polynomial_features.transform(x_lin.reshape(-1, 1))\n",
    "\n",
    "y_poly_pred = model_poly.predict(x_lin_poly)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "ax = plt.gca()\n",
    "ax.plot(x_lin, y_poly_pred, color='m', linewidth=2, label='polynomial model')\n",
    "plot_lorentz(df, x_test.index, ax=ax)\n",
    "ax.set_title(f\"The Lorentz Equation: Polynomial Fit, $R^2$ score = {r2_score(y_train, y_poly_train):.4f}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we get some wild behavior because we need a very high polynomial degree to begin to approximate this function.\n",
    "\n",
    "Let's see if we can do better using a **feedforward neural network** with PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Neural Networks in PyTorch\n",
    "\n",
    "In PyTorch, we define neural networks by creating a class that inherits from `nn.Module`. This gives us full control over the forward pass and allows for complex architectures.\n",
    "\n",
    "The basic pattern is:\n",
    "1. Define layers in `__init__`\n",
    "2. Define the forward pass in `forward`\n",
    "\n",
    "```python\n",
    "class MyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LorentzNN(nn.Module):\n",
    "    \"\"\"Simple feedforward neural network for Lorentz function regression.\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_sizes=[50, 50], activation='tanh'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Build layers\n",
    "        layers = []\n",
    "        input_size = 1\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(input_size, hidden_size))\n",
    "            if activation == 'tanh':\n",
    "                layers.append(nn.Tanh())\n",
    "            elif activation == 'relu':\n",
    "                layers.append(nn.ReLU())\n",
    "            input_size = hidden_size\n",
    "        \n",
    "        # Output layer (linear activation for regression)\n",
    "        layers.append(nn.Linear(input_size, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "# Create model and print summary\n",
    "model = LorentzNN(hidden_sizes=[50, 50])\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Training Loop\n",
    "\n",
    "In PyTorch, we write an explicit training loop, which gives us full control over the training process.\n",
    "\n",
    "The basic training loop:\n",
    "1. **Forward pass**: Compute predictions\n",
    "2. **Compute loss**: Compare predictions to targets\n",
    "3. **Backward pass**: Compute gradients via `loss.backward()`\n",
    "4. **Update weights**: Apply gradients via `optimizer.step()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val, \n",
    "                optimizer, criterion, epochs=100, verbose=True):\n",
    "    \"\"\"\n",
    "    Train a PyTorch model and return training history.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch nn.Module\n",
    "        X_train, y_train: Training data tensors\n",
    "        X_val, y_val: Validation data tensors\n",
    "        optimizer: PyTorch optimizer\n",
    "        criterion: Loss function\n",
    "        epochs: Number of training epochs\n",
    "        verbose: Print progress every 50 epochs\n",
    "    \n",
    "    Returns:\n",
    "        dict: Training history with 'loss' and 'val_loss' keys\n",
    "    \"\"\"\n",
    "    history = {'loss': [], 'val_loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X_train) # No mini-batching here\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_val_pred = model(X_val)\n",
    "            val_loss = criterion(y_val_pred, y_val)\n",
    "        \n",
    "        history['loss'].append(loss.item())\n",
    "        history['val_loss'].append(val_loss.item())\n",
    "        \n",
    "        if verbose and (epoch + 1) % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f} - Val Loss: {val_loss.item():.4f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for plotting training history\n",
    "def plot_history(history, title=None, ax=None):\n",
    "    \"\"\"Plot training and validation loss curves.\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.plot(history['loss'], label='train')\n",
    "    ax.plot(history['val_loss'], label='validation')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('MSE')\n",
    "    best_loss = np.nanmin(history['val_loss'])\n",
    "    best_epoch = np.nanargmin(history['val_loss'])\n",
    "    ax.axvline(best_epoch, c='k', ls='--', label=f'best val loss = {best_loss:.2f}')\n",
    "    ax.legend()\n",
    "    if title:\n",
    "        ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "X_train_t = torch.FloatTensor(x_train.values.reshape(-1, 1))\n",
    "y_train_t = torch.FloatTensor(y_train.values.reshape(-1, 1))\n",
    "X_test_t = torch.FloatTensor(x_test.values.reshape(-1, 1))\n",
    "y_test_t = torch.FloatTensor(y_test.values.reshape(-1, 1))\n",
    "\n",
    "print(f\"Training data shape: {X_train_t.shape}\")\n",
    "print(f\"Test data shape: {X_test_t.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Neural Network\n",
    "\n",
    "Now let's train our neural network on the Lorentz data and compare its fit to the polynomial baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the neural network\n",
    "torch.manual_seed(SEED)\n",
    "model = LorentzNN(hidden_sizes=[50, 50])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "print(model)\n",
    "\n",
    "history = train_model(model, X_train_t, y_train_t, X_test_t, y_test_t,\n",
    "                      optimizer, criterion, epochs=200, verbose=True)\n",
    "\n",
    "# Plot results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training history\n",
    "plot_history(history, title='Neural Network Training', ax=axes[0])\n",
    "\n",
    "# Predictions vs polynomial baseline\n",
    "model.eval()\n",
    "x_lin = np.linspace(df.x.min(), df.x.max(), 500).reshape(-1, 1)\n",
    "x_lin_t = torch.FloatTensor(x_lin)\n",
    "with torch.no_grad():\n",
    "    y_nn_pred = model(x_lin_t).numpy()\n",
    "\n",
    "# Recompute polynomial predictions on the same grid\n",
    "x_lin_poly_new = polynomial_features.transform(x_lin)\n",
    "y_poly_pred_new = model_poly.predict(x_lin_poly_new)\n",
    "\n",
    "axes[1].plot(x_lin, y_nn_pred, color='r', linewidth=2, label='neural network')\n",
    "axes[1].plot(x_lin, y_poly_pred_new, color='m', linewidth=2, alpha=0.5, label='polynomial (deg 25)')\n",
    "plot_lorentz(df, x_test.index, ax=axes[1])\n",
    "\n",
    "with torch.no_grad():\n",
    "      mse_nn = mean_squared_error(y_test, model(X_test_t).numpy())\n",
    "axes[1].set_title(f'Neural Network vs Polynomial (NN Test MSE = {mse_nn:.2f})')\n",
    "\n",
    "fig.tight_layout()\n",
    "print(f'Polynomial Test MSE: {mse_test_poly:.2f}')\n",
    "print(f'Neural Net Test MSE:  {mse_nn:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Weight Initializers\n",
    "\n",
    "How we initialize the weights of a neural network can significantly affect training. PyTorch provides several initialization methods in `torch.nn.init`.\n",
    "\n",
    "**Fan-in, Fan-out, and \"gain\"**\n",
    "\n",
    "Many initializers are written in terms of:\n",
    "\n",
    "- `fan_in`: number of input units to a layer\n",
    "- `fan_out`: number of output units from a layer\n",
    "- `gain`: a multiplier that depends on the activation function (e.g., ReLU vs tanh)\n",
    "\n",
    "**Common Initializers**\n",
    "\n",
    "The standard initializers you might expect are:\n",
    "- `nn.init.normal_` — Normal distribution\n",
    "- `nn.init.uniform_` — Uniform distribution\n",
    "- `nn.init.ones_` — All ones\n",
    "- `nn.init.zeros_` — All zeros\n",
    "\n",
    "Some more sophisticated initializers include:\n",
    "\n",
    "- **Xavier/Glorot normal** (`nn.init.xavier_normal_`)\n",
    "  - `std = gain * sqrt(2 / (fan_in + fan_out))`\n",
    "  - [Glorot et al., 2010](http://proceedings.mlr.press/v9/glorot10a.html)\n",
    "\n",
    "- **Xavier/Glorot uniform** (`nn.init.xavier_uniform_`)\n",
    "  - samples from `U[-limit, limit]` where  \n",
    "    `limit = gain * sqrt(6 / (fan_in + fan_out))`  \n",
    "\n",
    "    *(Here, `U[a, b]` denotes a uniform distribution on `[a, b]`.)*\n",
    "\n",
    "- **Kaiming/He normal** (`nn.init.kaiming_normal_`) *(commonly paired with ReLU / leaky ReLU)*\n",
    "  - `std = gain / sqrt(fan_in)`\n",
    "  - for ReLU, `gain = sqrt(2)` → `std = sqrt(2 / fan_in)`\n",
    "  - [He et al., 2015](https://arxiv.org/abs/1502.01852)\n",
    "\n",
    "- **Kaiming/He uniform** (`nn.init.kaiming_uniform_`) *(commonly paired with ReLU / leaky ReLU)*\n",
    "  - samples from `U[-limit, limit]` where  \n",
    "    `limit = sqrt(3) * std = sqrt(3) * gain / sqrt(fan_in)`\n",
    "  - for ReLU, this becomes `limit = sqrt(6 / fan_in)`\n",
    "\n",
    "**Note:** \n",
    "\n",
    "PyTorch default initialization for `nn.Linear`\n",
    "\n",
    "PyTorch’s `nn.Linear` **does** use `kaiming_uniform_` internally, but **not with the ReLU gain**.\n",
    "\n",
    "Specifically, PyTorch calls:\n",
    "\n",
    "`nn.init.kaiming_uniform_(weight, a = sqrt(5))`\n",
    "\n",
    "This choice makes the effective bound:\n",
    "\n",
    "`weight ~ U[-1/sqrt(fan_in), +1/sqrt(fan_in)]`\n",
    "\n",
    "So, **PyTorch uses Kaiming-uniform under the hood, but its default bound for `nn.Linear` is `± 1/sqrt(fan_in)`**, not `± sqrt(6/fan_in)` (which corresponds to ReLU gain).\n",
    "\n",
    "In practice (not automatically in PyTorch), Xavier/Glorot is commonly used when a linear layer is followed by tanh or sigmoid, while Kaiming/He is commonly used when a linear layer is followed by ReLU or leaky ReLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_with_init(hidden_sizes, init_fn):\n",
    "    \"\"\"\n",
    "    Create a model with a specific weight initialization.\n",
    "    \n",
    "    Args:\n",
    "        hidden_sizes: List of hidden layer sizes\n",
    "        init_fn: Initialization function to apply\n",
    "    \n",
    "    Returns:\n",
    "        nn.Module: Initialized model (LorentzNN)\n",
    "    \"\"\"\n",
    "    model = LorentzNN(hidden_sizes=hidden_sizes)\n",
    "    \n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            init_fn(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, 4, figsize=(16, 10))\n",
    "\n",
    "# Define initializers with their PyTorch equivalents\n",
    "initializers = {\n",
    "    'xavier_normal': lambda w: nn.init.xavier_normal_(w),\n",
    "    'xavier_uniform': lambda w: nn.init.xavier_uniform_(w),\n",
    "    'kaiming_uniform': lambda w: nn.init.kaiming_uniform_(w, nonlinearity='tanh'), ## gain adjusts for tanh  \n",
    "    'normal': lambda w: nn.init.normal_(w, mean=0, std=0.05)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for i, (init_name, init_fn) in enumerate(initializers.items()):\n",
    "    # Build and initialize model\n",
    "    torch.manual_seed(SEED)  # For reproducibility\n",
    "    model = create_model_with_init([200, 50, 5], init_fn)\n",
    "\n",
    "    if i == 0:\n",
    "        print(model)\n",
    "    # Get initial weights from first layer\n",
    "    initial_weights = model.network[0].weight.data.numpy().flatten()\n",
    "    axs[i, 2].hist(initial_weights, density=True, bins=30)\n",
    "    sns.kdeplot(initial_weights, ax=axs[i, 2])\n",
    "    axs[i, 2].set_xlabel('weight')\n",
    "    axs[i, 2].set_title('Initialization')\n",
    "    \n",
    "    # Train\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.MSELoss()\n",
    "    history = train_model(model, X_train_t, y_train_t, X_test_t, y_test_t,\n",
    "                         optimizer, criterion, epochs=100, verbose=False)\n",
    "    \n",
    "    # Plot history\n",
    "    plot_history(history, ax=axs[i, 0], title=init_name)\n",
    "    \n",
    "    # Plot predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_hat = model(X_test_t).numpy()\n",
    "    mse = mean_squared_error(y_test, y_hat)\n",
    "    results[init_name] = mse\n",
    "    x_lin = np.linspace(df.x.min(), df.x.max(), 500).reshape(-1, 1)\n",
    "    x_lin_t = torch.FloatTensor(x_lin)\n",
    "    with torch.no_grad():\n",
    "        y_hat_lin = model(x_lin_t).numpy()\n",
    "    axs[i, 1].plot(x_lin, y_hat_lin)\n",
    "    plot_lorentz(df, test_idx=x_test.index, ax=axs[i, 1])\n",
    "    axs[i, 1].set_title(f'Test MSE = {mse:.2f}')\n",
    "    \n",
    "    # Plot weights after training\n",
    "    final_weights = model.network[0].weight.data.numpy().flatten()\n",
    "    axs[i, 3].hist(final_weights, density=True, bins=30)\n",
    "    sns.kdeplot(final_weights, ax=axs[i, 3])\n",
    "    axs[i, 3].set_xlabel('weight')\n",
    "    axs[i, 3].set_title('After Training')\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "print(\"\\nTest MSE by Initializer:\")\n",
    "for name, mse in results.items():\n",
    "    print(f\"  {name:20s} {mse:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributions of the weights in the first layer after fitting seem to become multi-modal, with peaks on either side of 0. Why might this be the case?\n",
    "\n",
    "In any event, we can tell from the plots of the model predictions that we still have room for improvement. All these models seem to be **underfit**. More epochs could help, but let's first explore different options with our **optimizer**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **❓Question 1: Weight Initialization** \n",
    ">\n",
    "> 1. Why do the weight distributions become bimodal after training?\n",
    "> 2. What problems might occur if we initialized all weights to zero?\n",
    "> 3. Why might Kaiming/He initialization be preferred for ReLU networks?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Optimizers\n",
    "\n",
    "Recall that backpropagation uses the chain rule to calculate the gradient of the loss with respect to the weights. But it is gradient descent that actually updates the model weights. How this update is performed is defined by the optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From Lecture 2:** Gradient descent iteratively steps in the direction of the negative gradient to minimize the loss.\n",
    "\n",
    "<img src=\"./fig/lec02_gradient_descent_summary.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 1. Stochastic Gradient Descent (SGD)\n",
    "\n",
    "[`torch.optim.SGD`](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)\n",
    "\n",
    "```python\n",
    "torch.optim.SGD(params, lr=0.01, momentum=0, dampening=0, \n",
    "                weight_decay=0, nesterov=False)\n",
    "```\n",
    "\n",
    "In mini-batch SGD, the gradients are averaged across all $m$ observations in a mini-batch:\n",
    "\n",
    "$$g = \\frac{1}{m} \\sum_i \\nabla_W L(f(x_i; W), y_i)$$\n",
    "\n",
    "Then comes the update step where the weights are actually adjusted:\n",
    "\n",
    "$$W^* = W - \\eta g$$\n",
    "\n",
    "Here, $\\eta$ is our learning rate.\n",
    "\n",
    "Because the gradient is calculated using a random sample (mini-batch), it is a 'noisy' approximation which can allow us to escape local minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save initial weights to reload for fair comparison\n",
    "torch.manual_seed(SEED)\n",
    "base_model = LorentzNN(hidden_sizes=[50, 50])  # Default init Kaiming uniform\n",
    "initial_state = copy.deepcopy(base_model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with basic SGD\n",
    "model = LorentzNN(hidden_sizes=[50, 50])\n",
    "model.load_state_dict(copy.deepcopy(initial_state))\n",
    "\n",
    "# Compile equivalent: create optimizer and loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "history = train_model(model, X_train_t, y_train_t, X_test_t, y_test_t,\n",
    "                     optimizer, criterion, epochs=50, verbose=False)\n",
    "\n",
    "plot_history(history, title='SGD (lr=0.01, momentum=0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of Batch Size\n",
    "\n",
    "When performing stochastic gradient descent, batch size is another hyperparameter. It is, after all, what makes the optimizer \"stochastic\"! The smaller the batches, the noisier the approximations of the gradient.\n",
    "\n",
    "Let's see what happens with different batch sizes using PyTorch's DataLoader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From Lecture 2:** The mini-batch SGD algorithm — divide data into batches and update weights after each.\n",
    "\n",
    "<img src=\"./fig/lec02_minibatch_sgd_algorithm.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_batched(model, X_train, y_train, X_val, y_val,\n",
    "                        optimizer, criterion, batch_size, epochs=100, verbose=False):\n",
    "    \"\"\"\n",
    "    Train with mini-batches using DataLoader.\n",
    "    \"\"\"\n",
    "    dataset = TensorDataset(X_train, y_train)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    history = {'loss': [], 'val_loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_X, batch_y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch_X)\n",
    "            loss = criterion(y_pred, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * len(batch_X)\n",
    "        \n",
    "        epoch_loss /= len(X_train)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(X_val)\n",
    "            val_loss = criterion(val_pred, y_val).item()\n",
    "        \n",
    "        history['loss'].append(epoch_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        if verbose and (epoch + 1) % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {epoch_loss:.4f} - Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 8), sharex=True, sharey=True)\n",
    "axs = axs.flatten()\n",
    "batch_sizes = [128, 32, 8, 1]\n",
    "\n",
    "for ax, batch_size in zip(axs, batch_sizes):\n",
    "    torch.manual_seed(SEED)\n",
    "    model = LorentzNN(hidden_sizes=[50, 50])\n",
    "    model.load_state_dict(copy.deepcopy(initial_state))\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    history = train_model_batched(model, X_train_t, y_train_t, X_test_t, y_test_t,\n",
    "                                  optimizer, criterion, batch_size=batch_size, epochs=200)\n",
    "    \n",
    "    plot_history(history, title=f'Batch Size = {batch_size}', ax=ax)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "- batch size = 1: **vanilla SGD**. Just one example at a time to take a single step.\n",
    "- batch size = m: **mini-batch SGD**. Uses only the points in the mini-batch to calculate the loss function.\n",
    "- batch size = all data: **batch gradient descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of Learning Rate\n",
    "\n",
    "Vanilla SGD has a fixed learning rate. Let's see how adjusting it as a hyperparameter affects the performance of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From Lecture 2:** The effect of learning rate on gradient descent convergence.\n",
    "\n",
    "<img src=\"./fig/lec02_learning_rate_effects.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 8), sharex=True, sharey=True)\n",
    "axs = axs.flatten()\n",
    "lrs = [1e-4, 1e-3, 1e-2, 0.1]\n",
    "\n",
    "for ax, lr in zip(axs, lrs):\n",
    "    torch.manual_seed(SEED)\n",
    "    model = LorentzNN(hidden_sizes=[50, 50])\n",
    "    model.load_state_dict(copy.deepcopy(initial_state))\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    history = train_model_batched(model, X_train_t, y_train_t, X_test_t, y_test_t,\n",
    "                                  optimizer, criterion, batch_size=32, epochs=200)\n",
    "    \n",
    "    plot_history(history, title=f'Learning Rate = {lr}', ax=ax)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly some of these learning rates are too low, causing the loss to decrease very slowly as the weight updates are being scaled down considerably. The learning rate of 0.1 reaches a much lower loss, but it starts to oscillate wildly, apparently bouncing in and out of minima because of the high learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scheduling\n",
    "\n",
    "You can use a learning rate schedule to modulate how the learning rate of your optimizer changes over time.\n",
    "\n",
    "PyTorch provides schedulers in [`torch.optim.lr_scheduler`](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate).\n",
    "\n",
    "Common schedulers:\n",
    "- `StepLR`: Decay by gamma every step_size epochs\n",
    "- `ExponentialLR`: Decay by gamma every epoch\n",
    "- `ReduceLROnPlateau`: Reduce when metric stops improving\n",
    "- `CosineAnnealingLR`: Cosine annealing schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "# Exercise: Try an exponentially-decaying learning rate.\n",
    "# Use an initial learning rate of 0.1 and a decay factor (gamma) of 0.99\n",
    "# your code here\n",
    "torch.manual_seed(SEED)\n",
    "model = LorentzNN(hidden_sizes=[50, 50])\n",
    "model.load_state_dict(copy.deepcopy(initial_state))\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.99)  # Decays lr by 0.99 each epoch\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training with LR scheduler\n",
    "def train_with_scheduler(model, X_train, y_train, X_val, y_val,\n",
    "                         optimizer, scheduler, criterion, batch_size, epochs):\n",
    "    dataset = TensorDataset(X_train, y_train)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    history = {'loss': [], 'val_loss': [], 'lr': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_X, batch_y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch_X)\n",
    "            loss = criterion(y_pred, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * len(batch_X)\n",
    "        \n",
    "        epoch_loss /= len(X_train)\n",
    "        \n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(X_val)\n",
    "            val_loss = criterion(val_pred, y_val).item()\n",
    "        \n",
    "        history['loss'].append(epoch_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['lr'].append(current_lr)\n",
    "    \n",
    "    return history\n",
    "\n",
    "history = train_with_scheduler(model, X_train_t, y_train_t, X_test_t, y_test_t,\n",
    "                               optimizer, scheduler, criterion, batch_size=32, epochs=400)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "plot_history(history, title='Exponential LR Decay', ax=axs[0])\n",
    "axs[1].plot(history['lr'])\n",
    "axs[1].set_xlabel('epoch')\n",
    "axs[1].set_ylabel('learning rate')\n",
    "axs[1].set_title('Learning Rate Schedule')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 2. Adagrad\n",
    "\n",
    "[`torch.optim.Adagrad`](https://pytorch.org/docs/stable/generated/torch.optim.Adagrad.html)\n",
    "\n",
    "Adagrad is an optimizer with parameter-specific learning rates, which are adapted relative to how frequently a parameter gets updated during training. The more updates a parameter receives, the smaller the updates.\n",
    "\n",
    "<img src=\"./fig/adagrad_update_rule.png\" width=\"600\">\n",
    "\n",
    "A weight's learning rate is inversely proportional to the root of its accumulated squared component of the gradient:\n",
    "\n",
    "$$r^*_i = r_i + g^2_i$$\n",
    "\n",
    "\n",
    "$$W^*_i = W_i - \\frac{\\epsilon}{\\delta + \\sqrt{r^*_i}}g_i$$\n",
    "\n",
    "\n",
    "[Duchi et al., 2011](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "model = LorentzNN(hidden_sizes=[50, 50])\n",
    "model.load_state_dict(copy.deepcopy(initial_state))\n",
    "\n",
    "# Exercise: Create Adagrad optimizer with learning rate 0.01\n",
    "# your code here\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "history = train_model_batched(model, X_train_t, y_train_t, X_test_t, y_test_t,\n",
    "                              optimizer, criterion, batch_size=32, epochs=200)\n",
    "\n",
    "plot_history(history, title='Adagrad (lr=0.01)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there is a problem with this approach. The accumulated gradients can quickly shrink the learning rates to the point where the network is no longer learning anything.\n",
    "\n",
    "But we have a fix for this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3. RMSprop\n",
    "\n",
    "[`torch.optim.RMSprop`](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html)\n",
    "\n",
    "RMSprop maintains a moving (discounted) average of the square of gradients and divides the current gradient by the root of this average (RMS = root mean square).\n",
    "\n",
    "$$r^*_i = \\rho r_i + (1-\\rho)g^2_i$$\n",
    "\n",
    "$$W^*_i = W_i - \\frac{\\epsilon}{\\delta + \\sqrt{r^*_i}}g_i$$  \n",
    "\n",
    "[Hinton, 2012](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "model = LorentzNN(hidden_sizes=[50, 50])\n",
    "model.load_state_dict(copy.deepcopy(initial_state))\n",
    "\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.01, alpha=0.9, momentum=0)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "history = train_model_batched(model, X_train_t, y_train_t, X_test_t, y_test_t,\n",
    "                              optimizer, criterion, batch_size=32, epochs=200)\n",
    "\n",
    "plot_history(history, title='RMSprop (lr=0.01, alpha=0.9)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works well, but can we also have it use information about the general trajectory or \"trend\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4. SGD with Momentum\n",
    "\n",
    "If we incorporate information about **past updates**, might that help us converge faster?\n",
    "\n",
    "**Momentum** helps us move along with the general trajectory we've taken so far, with oscillations cancelling themselves out.\n",
    "\n",
    "<img src=\"./fig/momentum_visualization.png\" width=\"600\">\n",
    "\n",
    "Updates are made using a weighted average of the current gradient $g$ and the average \"trend\" seen so far $v$:\n",
    "\n",
    "$$v = \\alpha v + (1-\\alpha)g$$\n",
    "\n",
    "$$W^* = W - \\eta v$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 8), sharex=True)\n",
    "axs = axs.flatten()\n",
    "lrs = [1e-4, 1e-3, 1e-2, 0.1]\n",
    "\n",
    "for ax, lr in zip(axs, lrs):\n",
    "    torch.manual_seed(SEED)\n",
    "    model = LorentzNN(hidden_sizes=[50, 50])\n",
    "    model.load_state_dict(copy.deepcopy(initial_state))\n",
    "    \n",
    "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, nesterov=False)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    history = train_model_batched(model, X_train_t, y_train_t, X_test_t, y_test_t,\n",
    "                                  optimizer, criterion, batch_size=32, epochs=200)\n",
    "    \n",
    "    plot_history(history, title=f'Learning Rate = {lr} + Momentum', ax=ax)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened in the 4th example above? Let's train again with those parameters and look at the detailed output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect strange behavior in 4th plot above\n",
    "torch.manual_seed(SEED)\n",
    "model = LorentzNN(hidden_sizes=[50, 50])\n",
    "model.load_state_dict(copy.deepcopy(initial_state))\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Manual training to see each epoch\n",
    "dataset = TensorDataset(X_train_t, y_train_t)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "for epoch in range(15):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    \n",
    "    for batch_X, batch_y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(batch_X)\n",
    "        loss = criterion(y_pred, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item() * len(batch_X)\n",
    "    \n",
    "    epoch_loss /= len(X_train_t)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_pred = model(X_test_t)\n",
    "        val_loss = criterion(val_pred, y_test_t).item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/15 - loss: {epoch_loss:.4f} - val_loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the combination of high learning rate and momentum caused a numerical overflow, likely from hitting very 'steep' parts of the loss surface. It is bad news once you get a `nan` in your computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if weights became NaN\n",
    "for name, param in model.named_parameters():\n",
    "    if torch.isnan(param).any():\n",
    "        print(f\"{name}: Contains NaN values!\")\n",
    "    else:\n",
    "        print(f\"{name}: OK (min={param.min().item():.4f}, max={param.max().item():.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Clipping\n",
    "\n",
    "We can take steps to prevent outcomes like this. In PyTorch, we use `torch.nn.utils.clip_grad_norm_` or `clip_grad_value_` to prevent giant gradients (and thus giant weight updates) from causing problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_grad_clip(model, X_train, y_train, X_val, y_val,\n",
    "                         optimizer, criterion, batch_size, epochs, max_norm=1.0):\n",
    "    \"\"\"\n",
    "    Train with gradient clipping to prevent exploding gradients.\n",
    "    \"\"\"\n",
    "    dataset = TensorDataset(X_train, y_train)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    history = {'loss': [], 'val_loss': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for batch_X, batch_y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(batch_X)\n",
    "            loss = criterion(y_pred, batch_y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "            \n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * len(batch_X)\n",
    "        \n",
    "        epoch_loss /= len(X_train)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_pred = model(X_val)\n",
    "            val_loss = criterion(val_pred, y_val).item()\n",
    "        \n",
    "        history['loss'].append(epoch_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "model = LorentzNN(hidden_sizes=[50, 50])\n",
    "model.load_state_dict(copy.deepcopy(initial_state))\n",
    "\n",
    "# Exercise: Create SGD optimizer with lr=0.1, momentum=0.9\n",
    "# and use gradient clipping with max_norm=0.1\n",
    "# your code here\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "history = train_with_grad_clip(model, X_train_t, y_train_t, X_test_t, y_test_t,\n",
    "                               optimizer, criterion, batch_size=32, epochs=200, max_norm=0.1)\n",
    "\n",
    "plot_history(history, title='LR=0.1; momentum=0.9; grad_clip=0.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 5. Adam (Adaptive Moment Estimation)\n",
    "\n",
    "[`torch.optim.Adam`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html)\n",
    "\n",
    "Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments (basically, momentum + RMSprop).\n",
    "\n",
    "```python\n",
    "torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n",
    "```\n",
    "\n",
    "[Kingma et al., 2014](http://arxiv.org/abs/1412.6980)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "model = LorentzNN(hidden_sizes=[50, 50])\n",
    "model.load_state_dict(copy.deepcopy(initial_state))\n",
    "\n",
    "# Exercise: Create Adam optimizer with learning rate 0.01\n",
    "# your code here\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999))\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "history = train_model_batched(model, X_train_t, y_train_t, X_test_t, y_test_t,\n",
    "                              optimizer, criterion, batch_size=32, epochs=200)\n",
    "\n",
    "plot_history(history, title='Adam (lr=0.01)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very nice. Perhaps the lowest MSE we've seen so far. How does the fit look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lorentz(df, test_idx=x_test.index)\n",
    "ax = plt.gca()\n",
    "\n",
    "model.eval()\n",
    "x_lin = np.linspace(df.x.min(), df.x.max(), 500).reshape(-1, 1)\n",
    "x_lin_t = torch.FloatTensor(x_lin)\n",
    "with torch.no_grad():\n",
    "    y_hat = model(x_lin_t).numpy()\n",
    "ax.plot(x_lin, y_hat, c='m', label='NN prediction')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better than our original polynomial model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Other Optimizer Implementations Available in PyTorch:**\n",
    "- [`Adadelta`](https://pytorch.org/docs/stable/generated/torch.optim.Adadelta.html) - Improvement on Adagrad\n",
    "- [`Adamax`](https://pytorch.org/docs/stable/generated/torch.optim.Adamax.html) - Variant of Adam based on infinity norm\n",
    "- [`NAdam`](https://pytorch.org/docs/stable/generated/torch.optim.NAdam.html) - Adam with Nesterov momentum\n",
    "- [`AdamW`](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) - Adam with decoupled weight decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **❓ Question 2: Optimizers**\n",
    ">\n",
    "> 1. What is the key difference between SGD and Adam?\n",
    "> 2. Why might gradient clipping be important when using high learning rates with momentum?\n",
    "> 3. In what scenarios might you prefer SGD over Adam?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Overfitting\n",
    "\n",
    "So far we've been looking at an oversimplified dataset where all the datapoints are right on top of the true generating function. But the real world is noisy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('data/lorentz_noise_set2.csv')\n",
    "df2 = df2.sample(frac=1, random_state=SEED)  # shuffle DataFrame\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(\n",
    "    df2.x, df2.y, train_size=0.7, random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lorentz(df2, x_test2.index)\n",
    "plt.title('Lorentz Data with Noise');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert noisy data to tensors\n",
    "X_train2_t = torch.FloatTensor(x_train2.values.reshape(-1, 1))\n",
    "y_train2_t = torch.FloatTensor(y_train2.values.reshape(-1, 1))\n",
    "X_test2_t = torch.FloatTensor(x_test2.values.reshape(-1, 1))\n",
    "y_test2_t = torch.FloatTensor(y_test2.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does our previously best performing model fare on this more realistic dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "torch.manual_seed(SEED)\n",
    "model_overfit = LorentzNN(hidden_sizes=[50, 50])\n",
    "\n",
    "optimizer = optim.Adam(model_overfit.parameters(), lr=0.01, betas=(0.9, 0.999))\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "history = train_model_batched(model_overfit, X_train2_t, y_train2_t, X_test2_t, y_test2_t,\n",
    "                              optimizer, criterion, batch_size=32, epochs=2000, verbose=False)\n",
    "\n",
    "plot_history(history, title='Training on Noisy Data (2000 epochs)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are clear signs of overfitting as the validation error starts to diverge from the train error. Any \"improvement\" seen with respect to the training data after a certain point no longer generalizes. And after a while, we actually start to see the validation loss increasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the overfit predictions\n",
    "plot_lorentz(df2, test_idx=x_test2.index)\n",
    "ax = plt.gca()\n",
    "\n",
    "model_overfit.eval()\n",
    "x_lin = np.linspace(df2.x.min(), df2.x.max(), 500).reshape(-1, 1)\n",
    "x_lin_t = torch.FloatTensor(x_lin)\n",
    "with torch.no_grad():\n",
    "    y_hat = model_overfit(x_lin_t).numpy()\n",
    "ax.plot(x_lin, y_hat, c='m', label='NN prediction')\n",
    "ax.legend()\n",
    "ax.set_title('Overfit Model Predictions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we can see the model predictions jump around as it tries to fit the sparse and noisy points in the training data.\n",
    "\n",
    "Luckily we have several tools at our disposal for addressing overfitting in neural networks... to be continued!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **❓ Question 3: Overfitting**\n",
    ">\n",
    "> 1. At approximately which epoch does overfitting begin (where val_loss starts increasing)?\n",
    "> 2. What techniques might help prevent overfitting? (We'll cover these in future labs)\n",
    "> 3. Why does the model create \"wiggly\" predictions when overfit?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Image Classification with Feedforward Neural Networks\n",
    "\n",
    "In this section we will be working with the [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist).\n",
    "\n",
    "<img src=\"./fig/fashion_mnist_examples.jpg\" width=\"400px\" />\n",
    "\n",
    "Fashion-MNIST is a dataset of Zalando's article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from one of 10 classes.\n",
    "\n",
    "We will be using a very small fraction of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Download Fashion-MNIST dataset\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=True, download=True, transform=transform\n",
    ")\n",
    "test_dataset = torchvision.datasets.FashionMNIST(\n",
    "    root='./data', train=False, download=True, transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion-MNIST class labels\n",
    "labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "          'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "label2idx = {label: idx for idx, label in enumerate(labels)}\n",
    "print(label2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View some examples\n",
    "fig, axs = plt.subplots(3, 5, figsize=(9, 7))\n",
    "for i, ax in enumerate(axs.ravel()):\n",
    "    image, label = train_dataset[i]\n",
    "    ax.imshow(image.squeeze(), cmap=plt.cm.gray)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(labels[label])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset size\n",
    "print(f\"Full training set: {len(train_dataset)} images\")\n",
    "print(f\"Full test set: {len(test_dataset)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we'd like a model we can train multiple times as we experiment during the section, we will use a subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "# Use a subset of the data for faster training\n",
    "np.random.seed(SEED)\n",
    "train_indices = np.random.choice(len(train_dataset), size=1200, replace=False)\n",
    "test_indices = np.random.choice(len(test_dataset), size=1000, replace=False)\n",
    "\n",
    "# Split into train and validation\n",
    "train_idx, val_idx = train_indices[:960], train_indices[960:]\n",
    "\n",
    "train_subset = Subset(train_dataset, train_idx)\n",
    "val_subset = Subset(train_dataset, val_idx)\n",
    "test_subset = Subset(test_dataset, test_indices)\n",
    "\n",
    "print(f\"Training subset: {len(train_subset)} images\")\n",
    "print(f\"Validation subset: {len(val_subset)} images\")\n",
    "print(f\"Test subset: {len(test_subset)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Classifier Neural Network\n",
    "\n",
    "Let's build our first attempt at a clothing classifier and try to overfit.\n",
    "\n",
    "Note that PyTorch has a `nn.Flatten()` layer! We can use this to automatically turn input images into 1D arrays.\n",
    "(We'll see how to handle 2D input with CNNs in future lectures and sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Feedforward neural network for Fashion-MNIST classification.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=28*28, hidden_dims=[256, 128], num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = [nn.Flatten()]  # Flatten 28x28 images to 784-dim vectors\n",
    "        \n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Output layer (no softmax - CrossEntropyLoss handles it)\n",
    "        layers.append(nn.Linear(prev_dim, num_classes))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Create your classifier model\n",
    "# Experiment with different hidden_dims like [256, 128], [512, 256, 128], etc.\n",
    "\n",
    "\n",
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model, train_loader, val_loader, optimizer, criterion, \n",
    "                     epochs=30, patience=5):\n",
    "    \"\"\"\n",
    "    Train a classifier with early stopping.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        train_loader: Training DataLoader\n",
    "        val_loader: Validation DataLoader\n",
    "        optimizer: PyTorch optimizer\n",
    "        criterion: Loss function\n",
    "        epochs: Maximum number of epochs\n",
    "        patience: Early stopping patience\n",
    "    \n",
    "    Returns:\n",
    "        dict: Training history\n",
    "    \"\"\"\n",
    "    history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}\n",
    "    best_val_loss = float('inf')\n",
    "    best_weights = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for images, labels_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            train_total += labels_batch.size(0)\n",
    "            train_correct += (predicted == labels_batch).sum().item()\n",
    "        \n",
    "        train_loss /= train_total\n",
    "        train_acc = train_correct / train_total\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels_batch in val_loader:\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels_batch)\n",
    "                \n",
    "                val_loss += loss.item() * images.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels_batch.size(0)\n",
    "                val_correct += (predicted == labels_batch).sum().item()\n",
    "        \n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct / val_total\n",
    "        \n",
    "        history['loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['accuracy'].append(train_acc)\n",
    "        history['val_accuracy'].append(val_acc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - \"\n",
    "              f\"loss: {train_loss:.4f} - acc: {train_acc:.4f} - \"\n",
    "              f\"val_loss: {val_loss:.4f} - val_acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"\\nEarly stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "    # Restore best weights\n",
    "    if best_weights is not None:\n",
    "        model.load_state_dict(best_weights)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train the classifier\n",
    "torch.manual_seed(SEED)\n",
    "model_classifier = FashionClassifier(hidden_dims=[256, 128])\n",
    "\n",
    "optimizer = optim.Adam(model_classifier.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "history = train_classifier(model_classifier, train_loader, val_loader,\n",
    "                           optimizer, criterion, epochs=30, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(model, data_loader):\n",
    "    \"\"\"Evaluate model on a DataLoader.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels_batch in data_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels_batch)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels_batch.size(0)\n",
    "            correct += (predicted == labels_batch).sum().item()\n",
    "    \n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "train_loss, train_acc = evaluate_classifier(model_classifier, train_loader)\n",
    "val_loss, val_acc = evaluate_classifier(model_classifier, val_loader)\n",
    "test_loss, test_acc = evaluate_classifier(model_classifier, test_loader)\n",
    "\n",
    "print(f\"Train - Loss: {train_loss:.4f}, Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Val   - Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}\")\n",
    "print(f\"Test  - Loss: {test_loss:.4f}, Accuracy: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "best_epoch = np.argmin(history['val_loss'])\n",
    "print(f\"Best Val Accuracy: {history['val_accuracy'][best_epoch]:.2f} (epoch {best_epoch + 1})\")\n",
    "print(f\"Final Val Accuracy (last epoch): {history['val_accuracy'][-1]:.2f}\")\n",
    "\n",
    "# Accuracy plot\n",
    "axs[0].plot(history['accuracy'], label='train')\n",
    "axs[0].plot(history['val_accuracy'], label='validation')\n",
    "axs[0].set_title('Model Accuracy')\n",
    "axs[0].set_ylabel('Accuracy')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].legend(loc='lower right')\n",
    "\n",
    "# Loss plot\n",
    "axs[1].plot(history['loss'], label='train')\n",
    "axs[1].plot(history['val_loss'], label='validation')\n",
    "axs[1].set_title('Model Loss')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section we'll look at regularization techniques and data augmentation which will allow us to improve generalization!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **❓ Question 4: Classification**\n",
    ">\n",
    "> 1. Why do we use `nn.CrossEntropyLoss()` instead of `nn.MSELoss()` for classification?\n",
    "> 2. Why don't we need a softmax layer at the end of our classifier?\n",
    "> 3. What accuracy would you expect from random guessing on this 10-class problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🏋️ TEAM ACTIVITY: Optimizer Comparison\n",
    "\n",
    "Compare the performance of different optimizers on the Fashion-MNIST classification task.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train with SGD**\n",
    "\n",
    "Train the `FashionClassifier` using SGD with a few different learning rate and momentum combinations. Store each history so we can compare later.\n",
    "\n",
    "**Hint:** Remember to re-initialize the model and set `torch.manual_seed(SEED)` before each run for a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train with RMSprop**\n",
    "\n",
    "Train with `RMSprop`. You can experiment with the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train with Adam**\n",
    "\n",
    "Train with `Adam` using a few different learning rates. Adam was already used earlier in Part 5 — how do other learning rates compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare Training Curves**\n",
    "\n",
    "Plot the validation accuracy curves for all optimizers on a single figure. Which optimizer converges fastest? Which achieves the best final accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End of team activity**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
