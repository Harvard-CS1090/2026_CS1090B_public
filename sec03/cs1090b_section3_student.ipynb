{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4633457e",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Harvard-CS1090/2026_CS1090B_public/blob/main/sec03/cs1090b_sec03_solutions.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\">\n",
    "\n",
    "# CS1090B Section 3: Regularization\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Spring 2026**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, and Chris Gumb<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9a5dc0",
   "metadata": {},
   "source": [
    "## Setup: Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f50eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment detection and setup\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "# Define the zip file URL and expected directories\n",
    "assets_zip_url = \"https://github.com/Harvard-CS1090/2026_CS1090B_public/raw/main/sec03/notebook_assets.zip\"\n",
    "\n",
    "assets_zip_name = \"notebook_assets.zip\"\n",
    "expected_dirs = [\"data\", \"fig\"]\n",
    "\n",
    "# Check if required directories already exist\n",
    "all_dirs_exist = all(os.path.isdir(d) for d in expected_dirs)\n",
    "\n",
    "if all_dirs_exist:\n",
    "    print(\"Required directories already exist. Skipping download.\")\n",
    "else:\n",
    "    print(f\"Downloading {assets_zip_name} from GitHub...\")\n",
    "    \n",
    "    # Use wget in Colab, or urllib for local\n",
    "    try:\n",
    "        if 'google.colab' in sys.modules:\n",
    "            subprocess.run(['wget', '-q', assets_zip_url], check=True)\n",
    "        else:\n",
    "            import urllib.request\n",
    "            urllib.request.urlretrieve(assets_zip_url, assets_zip_name)\n",
    "        print(f\"Downloaded {assets_zip_name}.\")\n",
    "        \n",
    "        # Unzip the file\n",
    "        import zipfile\n",
    "        with zipfile.ZipFile(assets_zip_name, 'r') as zip_ref:\n",
    "            zip_ref.extractall('.')\n",
    "        print(f\"Extracted {assets_zip_name}.\")\n",
    "        \n",
    "        # Clean up the zip file\n",
    "        os.remove(assets_zip_name)\n",
    "        print(f\"Removed {assets_zip_name}.\")\n",
    "        \n",
    "        # Remove __MACOSX folder if it exists\n",
    "        if os.path.isdir('__MACOSX'):\n",
    "            shutil.rmtree('__MACOSX')\n",
    "            print(\"Removed __MACOSX folder.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during setup: {e}\", file=sys.stderr)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d3a337",
   "metadata": {},
   "source": [
    "## Part 1: Regularization for Regression\n",
    "\n",
    "When a model fits the training data too closely - capturing noise rather than the underlying pattern - it **overfits**, performing well on training examples but poorly on unseen data. Regularization techniques address this by constraining the model in various ways.\n",
    "\n",
    "In this section, we'll intentionally overfit a neural network on a regression task, then apply five regularization techniques to combat it: early stopping, L1/L2 weight penalties, dropout, batch normalization, and data augmentation via Gaussian noise. We'll compare how each method affects the training and validation loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffba45a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import random as rn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from torchvision import datasets, transforms \n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "# Reproducibility\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(109)\n",
    "rn.seed(109)\n",
    "torch.manual_seed(109);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d04b914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Lorentz dataset\n",
    "\n",
    "df = pd.read_csv('data/lorentz_noise_set2.csv')\n",
    "df = df.sample(frac=1, random_state=109).reset_index(drop=True)\n",
    "\n",
    "# Split first to avoid leakage, then fit scaler on training data only\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    df['x'],\n",
    "    df['y'],\n",
    "    train_size=0.7,\n",
    "    random_state=109\n",
    ")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_std = scaler.fit_transform(x_train.to_frame()).ravel()\n",
    "x_val_std = scaler.transform(x_val.to_frame()).ravel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03jkafac38ku",
   "metadata": {},
   "source": [
    "**Dataset:** We're using a synthetic dataset based on a Lorentz curve - a peaked, bell-shaped function that's tricky to fit. The exact physics doesn't matter here; what matters is that the relationship between input and output is nonlinear and noisy, making it a good testbed for overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944ba2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lorentz(df, idx, ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    train_mask = np.ones(df.shape[0], dtype=bool)\n",
    "    train_mask[idx] = False\n",
    "    ax.scatter(df.x[train_mask], df.y[train_mask], c='b', label='train data')\n",
    "    ax.scatter(df.x[~train_mask], df.y[~train_mask], c='orange', marker='^', label='Validation data')\n",
    "    ax.set_xlabel(r'$\\omega$ - x')\n",
    "    ax.set_ylabel(r'$\\epsilon$ - y')\n",
    "    ax.legend()\n",
    "\n",
    "plot_lorentz(df, idx=x_val.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71f2111",
   "metadata": {},
   "source": [
    "We will start with a small neural network and intentionally overfit. Below we define a flexible `RegressionMLP` model class and reusable training/plotting utilities that we'll use throughout Part 1 to compare different regularization strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba117f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class will be utilized later when we add Gaussian noise as a model layer.\n",
    "class GaussianNoise(nn.Module):\n",
    "    def __init__(self, stddev):\n",
    "        super().__init__()\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and self.stddev > 0:\n",
    "            noise = torch.randn_like(x) * self.stddev\n",
    "            return x + noise\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3874b133",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionMLP(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_sizes=(64, 128), dropout=0.0, use_batchnorm=False, noise_std=0.0):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        if noise_std > 0:\n",
    "            layers.append(GaussianNoise(noise_std))\n",
    "        in_dim = input_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_dim, h))\n",
    "            if use_batchnorm:\n",
    "                layers.append(nn.BatchNorm1d(h))\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            in_dim = h\n",
    "        layers.append(nn.Linear(in_dim, 1))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fi4tri1tqal",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regression(model, train_loader, val_loader, optimizer, criterion, epochs=200, l1_lambda=0.0, early_stopping=None):\n",
    "    history = {'loss': [], 'val_loss': []}\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb).squeeze(1)\n",
    "            loss = criterion(preds, yb)\n",
    "            if l1_lambda > 0:\n",
    "                # L1 penalty on weights only (not biases) ‚Äî standard convention\n",
    "                l1_penalty = sum(p.abs().sum() for n, p in model.named_parameters() if 'weight' in n)\n",
    "                loss = loss + l1_lambda * l1_penalty\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Note: when l1_lambda > 0, train_loss includes the L1 penalty\n",
    "            # while val_loss below is pure MSE, so the two curves aren't\n",
    "            # directly comparable for L1 runs.\n",
    "            train_loss += loss.item() * xb.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                preds = model(xb).squeeze(1)\n",
    "                loss = criterion(preds, yb)\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        history['loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "\n",
    "        if early_stopping and early_stopping.step(val_loss, model):\n",
    "            break\n",
    "\n",
    "    if early_stopping:\n",
    "        early_stopping.restore(model)\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def plot_history(history, title=None, ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    ax.plot(history['loss'], label='train')\n",
    "    ax.plot(history['val_loss'], label='validation')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('MSE')\n",
    "    best_loss = np.nanmin(history['val_loss'])\n",
    "    ax.axvline(np.nanargmin(history['val_loss']), c='k', ls='--', label=f'best val loss = {best_loss:.2f}')\n",
    "    ax.legend()\n",
    "    if title:\n",
    "        ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13922c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare tensors and loaders\n",
    "\n",
    "x_train_t = torch.tensor(x_train_std, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "y_train_t = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "x_val_t = torch.tensor(x_val_std, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "y_val_t = torch.tensor(y_val.values, dtype=torch.float32).to(device)\n",
    "\n",
    "train_ds = torch.utils.data.TensorDataset(x_train_t, y_train_t)\n",
    "val_ds = torch.utils.data.TensorDataset(x_val_t, y_val_t)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f179b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model (intentionally overfit)\n",
    "torch.manual_seed(109)\n",
    "model_base = RegressionMLP().to(device)\n",
    "optimizer = optim.Adam(model_base.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "history_base = train_regression(model_base, train_loader, val_loader, optimizer, criterion, epochs=500)\n",
    "plot_history(history_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6123f4a2",
   "metadata": {},
   "source": [
    "> **‚ùì Question 1: Diagnosing Overfitting**\n",
    ">\n",
    "> 1. What do you notice about the gap between training and validation loss as epochs increase?\n",
    "> 2. Why does the validation loss start to increase even as training loss continues to decrease?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2ff867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the overfit predictions\n",
    "plot_lorentz(df, idx=x_val.index)\n",
    "\n",
    "def plot_predictions(model):\n",
    "    model.eval()\n",
    "    x_lin = np.linspace(df.x.min(), df.x.max(), 500).reshape(-1, 1)\n",
    "    x_lin_std = scaler.transform(pd.DataFrame(x_lin, columns=['x'])) \n",
    "    with torch.no_grad():\n",
    "        x_tensor = torch.tensor(x_lin_std, dtype=torch.float32).to(device)\n",
    "        preds = model(x_tensor).detach().cpu().numpy()\n",
    "    ax = plt.gca()\n",
    "    ax.plot(x_lin, preds, label='prediction')                                                                                 \n",
    "    ax.legend();  \n",
    "\n",
    "plot_predictions(model_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77816e8d",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "\n",
    "Early stopping monitors validation loss during training and halts the process when performance stops improving. The idea is simple: as training continues, the model begins to memorize noise in the training data, causing validation loss to rise even as training loss keeps falling. By saving the best model and stopping once validation loss hasn't improved for a set number of epochs (*patience*), we get a model that generalizes better without needing to tune a regularization hyperparameter directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dx0s7w4un6u",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=50, min_delta=0.0, restore_best=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best = restore_best\n",
    "        self.best_loss = np.inf\n",
    "        self.best_state = None\n",
    "        self.counter = 0\n",
    "\n",
    "    def step(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if self.restore_best:\n",
    "                self.best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "    def restore(self, model):\n",
    "        if self.restore_best and self.best_state is not None:\n",
    "            model.load_state_dict(self.best_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1232d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping setup\n",
    "# patience=40: allow 40 epochs without improvement before stopping.\n",
    "# Too low and training halts prematurely; too high and we waste epochs overfitting.\n",
    "torch.manual_seed(109)\n",
    "early_stop = EarlyStopping(patience=40, restore_best=True)\n",
    "model_es = RegressionMLP().to(device)\n",
    "optimizer = optim.Adam(model_es.parameters(), lr=0.01)\n",
    "\n",
    "history_es = train_regression(\n",
    "    model_es,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    epochs=1000,\n",
    "    early_stopping=early_stop\n",
    ")\n",
    "plot_history(history_es, title='Early Stopping')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50519748",
   "metadata": {},
   "source": [
    "### Weight Decay (L2) and L1 Regularization\n",
    "\n",
    "Weight penalties add a cost to having large weights, discouraging the model from fitting noise. **L2 regularization** (weight decay) adds $\\lambda \\sum w^2$ to the loss, which shrinks all weights toward zero but rarely makes them exactly zero. **L1 regularization** adds $\\lambda \\sum |w|$ to the loss, which encourages *sparsity* - pushing many weights to exactly zero while keeping a few large ones. In practice, L2 produces smoother models while L1 can act as a form of feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915b0f3e",
   "metadata": {},
   "source": [
    "**From Lecture 4:** L2 regularization shrinks all weights uniformly toward zero, while L1 encourages sparsity by pushing some weights to exactly zero.\n",
    "\n",
    "<img src=\"./fig/lec04_l1_vs_l2_regularization.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fc9d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 regularization via weight_decay (applied to weights only, not biases)\n",
    "# weight_decay=0.005: controls the strength of the L2 penalty.\n",
    "# Larger values shrink weights more aggressively (smoother model, risk underfitting);\n",
    "# smaller values give weaker regularization (closer to baseline, risk overfitting).\n",
    "torch.manual_seed(109)\n",
    "model_l2 = RegressionMLP().to(device)\n",
    "optimizer = optim.Adam([\n",
    "    {'params': [p for n, p in model_l2.named_parameters() if 'weight' in n], 'weight_decay': 0.005},\n",
    "    {'params': [p for n, p in model_l2.named_parameters() if 'bias' in n], 'weight_decay': 0.0}\n",
    "], lr=0.01)\n",
    "\n",
    "history_l2 = train_regression(\n",
    "    model_l2,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    epochs=1500,\n",
    "    early_stopping=EarlyStopping(patience=200)\n",
    ")\n",
    "plot_history(history_l2, title='L2 Regularization')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5uwzb3qxufw",
   "metadata": {},
   "source": [
    "**Note:** PyTorch also provides `torch.optim.AdamW`, which applies weight decay *directly* to the weights rather than through the gradient. In standard `Adam`, the penalty gets scaled down by the adaptive learning rate, weakening it for frequently-updated parameters. `AdamW` avoids this and is preferred in modern practice (e.g., transformer fine-tuning). For this small example the difference is negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2663f67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 regularization via explicit penalty\n",
    "# l1_lambda=0.005: controls sparsity pressure.\n",
    "# Larger values push more weights to exactly zero (sparser model);\n",
    "# smaller values allow more non-zero weights (less feature selection).\n",
    "torch.manual_seed(109)\n",
    "model_l1 = RegressionMLP().to(device)\n",
    "optimizer = optim.Adam(model_l1.parameters(), lr=0.01)\n",
    "\n",
    "history_l1 = train_regression(\n",
    "    model_l1,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    epochs=1500,\n",
    "    l1_lambda=0.005,\n",
    "    early_stopping=EarlyStopping(patience=200)\n",
    ")\n",
    "plot_history(history_l1, title='L1 Regularization')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10077b2",
   "metadata": {},
   "source": [
    "> **‚ùì Question 2: L1 vs L2**\n",
    ">\n",
    "> 1. Which penalty tends to push more weights toward zero?\n",
    "> 2. How might that affect model complexity and interpretability?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62012d85",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "Dropout randomly zeroes a fraction of neuron activations during each training step, forcing the network to not rely on any single neuron. This acts as an implicit ensemble ‚Äî on each forward pass, a different \"thinned\" subnetwork is active, and the final model approximates an average over all these subnetworks. At evaluation time, dropout is turned off and all neurons are active (with outputs scaled accordingly). The `dropout` parameter controls what fraction of activations are dropped (e.g., 0.3 means 30% are zeroed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc765cd0",
   "metadata": {},
   "source": [
    "**From Lecture 4:** Dropout randomly \"drops\" neurons during training, forcing the network to learn redundant representations and reducing co-adaptation between neurons.\n",
    "\n",
    "<img src=\"./fig/lec04_dropout_mechanism.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a83dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout=0.3: randomly zero 30% of activations each forward pass.\n",
    "# Higher dropout (e.g., 0.5) gives stronger regularization but may slow convergence;\n",
    "# lower dropout (e.g., 0.1) has a milder effect.\n",
    "torch.manual_seed(109)\n",
    "model_dropout = RegressionMLP(dropout=0.3).to(device)\n",
    "optimizer = optim.Adam(model_dropout.parameters(), lr=0.01)\n",
    "\n",
    "history_dropout = train_regression(\n",
    "    model_dropout,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    epochs=1500,\n",
    "    early_stopping=EarlyStopping(patience=200)\n",
    ")\n",
    "plot_history(history_dropout, title='Dropout Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dbcd86",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n",
    "Batch normalization normalizes each layer's pre-activations to zero mean and unit variance (computed per mini-batch), then applies a learnable scale and shift. This stabilizes the distribution of inputs to each layer during training, which helps with optimization - allowing higher learning rates and reducing sensitivity to initialization. While not designed as a regularizer, the noise introduced by batch level statistics acts as a mild regularizer in practice.\n",
    "\n",
    "**Important caveat:** BatchNorm is primarily an **optimization technique**, not a regularizer. It shines in deep networks trained on large datasets with large batch sizes, where the mini-batch statistics are reliable estimates of the true distribution. On small datasets with small batches (like ours), those statistics are noisy, and the normalization can hurt more than it helps ‚Äî as we'll see below. \n",
    "\n",
    "BatchNorm can be more effective with larger datasets and batch sizes. In Part 2 we only use a small subset (1,200 samples), so the batch statistics can still be noisy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90016287",
   "metadata": {},
   "source": [
    "**From Lecture 4:** Internal Covariance Shift occurs when the distribution of layer inputs changes across batches, making training unstable. Batch normalization addresses this by normalizing activations.\n",
    "\n",
    "<img src=\"./fig/lec04_internal_covariance_shift.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37260b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same architecture as baseline, but with BatchNorm added.\n",
    "# No dropout ‚Äî we want to isolate BatchNorm's effect.\n",
    "torch.manual_seed(109)\n",
    "model_bn = RegressionMLP(hidden_sizes=(64, 128), use_batchnorm=True).to(device)\n",
    "optimizer = optim.Adam(model_bn.parameters(), lr=0.01)\n",
    "\n",
    "history_bn = train_regression(\n",
    "    model_bn,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    epochs=2000,\n",
    "    early_stopping=EarlyStopping(patience=200)\n",
    ")\n",
    "plot_history(history_bn, title='Batchnorm Model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23024c2",
   "metadata": {},
   "source": [
    "> **‚ùì Question 3: Batch Normalization**\n",
    ">\n",
    "> 1. Why does BatchNorm underperform on this small regression task compared to the other regularization methods?\n",
    "> 2. In what kinds of settings (dataset size, network depth, batch size) would you expect BatchNorm to be more effective?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89841934",
   "metadata": {},
   "source": [
    "### Data Augmentation with Gaussian Noise\n",
    "\n",
    "Adding random noise to inputs during training is a simple form of data augmentation that acts as a regularizer. Each time the model sees a training example, it sees a slightly perturbed version, which prevents it from memorizing exact input-output mappings. The noise is only applied during training - at evaluation time, the original clean inputs are used. This is especially useful when the dataset is small and collecting more data isn't feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ce99a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise_std=0.02: add Gaussian noise with std=0.02 to inputs during training.\n",
    "# Larger noise acts as stronger regularization but can obscure the signal;\n",
    "# smaller noise has a subtler effect. The right scale depends on the input range\n",
    "# (here inputs are standardized, so 0.02 is a small perturbation).\n",
    "torch.manual_seed(109)\n",
    "model_noise = RegressionMLP(noise_std=0.02).to(device)\n",
    "optimizer = optim.Adam(model_noise.parameters(), lr=0.01)\n",
    "\n",
    "history_noise = train_regression(\n",
    "    model_noise,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    epochs=1500,\n",
    "    early_stopping=EarlyStopping(patience=200)\n",
    ")\n",
    "plot_history(history_noise, title='Gaussian Noise Augmentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ypk9gisak",
   "metadata": {},
   "source": [
    "### Comparison of Regularization Techniques\n",
    "\n",
    "Let's compare all the approaches we've tried. For each method we report the **best validation MSE** (primary metric: how well does it generalize?) and the **train-val gap at that epoch** (how much is it overfitting at its best point?). The ideal is low val MSE *and* a small gap - a small gap alone isn't enough if the model isn't fitting the data well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g4ajvni4qji",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: exact metrics can vary across runs and hardware (CPU vs GPU) due to nondeterminism.\n",
    "results = {\n",
    "    'Baseline':       history_base,\n",
    "    'Early Stopping': history_es,\n",
    "    'L2 (weight decay)': history_l2,\n",
    "    'L1':             history_l1,\n",
    "    'Dropout':        history_dropout,\n",
    "    'BatchNorm':      history_bn,\n",
    "    'Gaussian Noise': history_noise,\n",
    "}\n",
    "\n",
    "names = list(results.keys())\n",
    "best_val  = [np.nanmin(h['val_loss']) for h in results.values()]\n",
    "best_val_epoch = [np.nanargmin(h['val_loss']) for h in results.values()]\n",
    "train_at_best = [h['loss'][e] for h, e in zip(results.values(), best_val_epoch)]\n",
    "\n",
    "# --- Table ---\n",
    "print(f\"{'Method':<20s} {'Best Val MSE':>12s} {'Train at Best':>14s} {'Gap':>8s}\")\n",
    "print(\"-\" * 58)\n",
    "for name, bv, tb in zip(names, best_val, train_at_best):\n",
    "    print(f\"{name:<20s} {bv:12.4f} {tb:14.4f} {bv - tb:8.4f}\")\n",
    "\n",
    "# --- Bar chart ---\n",
    "x = np.arange(len(names))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.bar(x - width/2, train_at_best, width, label='Train MSE at Best Val Epoch')\n",
    "ax.bar(x + width/2, best_val, width, label='Best Val MSE')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(names, rotation=30, ha='right')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_title('Regularization Comparison ‚Äî Part 1')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94797d1",
   "metadata": {},
   "source": [
    "## Part 2: Classification with Fashion-MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7a1b33",
   "metadata": {},
   "source": [
    "Having seen regularization in the regression setting, we now apply these ideas to a classification task. We'll build an MLP classifier on Fashion-MNIST, observe overfitting with limited data, and then use data augmentation to improve generalization.\n",
    "\n",
    "---\n",
    "\n",
    "Fashion-MNIST is a dataset of Zalando's article images‚Äîconsisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Fashion-MNIST is intended to serve as a drop-in replacement for MNIST.\n",
    "\n",
    "<img src=\"https://4.bp.blogspot.com/-OQZGt_5WqDo/Wa_Dfa4U15I/AAAAAAAAAUI/veRmAmUUKFA19dVw6XCOV2YLO6n-y_omwCLcBGAs/s400/out.jpg\" width=\"400px\" alt=\"Grid of Fashion-MNIST sample images showing 10 clothing categories: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, and Ankle boot\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06b62fd",
   "metadata": {},
   "source": [
    "### DataLoaders (PyTorch)\n",
    "\n",
    "We use `torchvision.datasets.FashionMNIST` with `transforms.Compose` and PyTorch DataLoaders. We'll take a small train/validation split for faster experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6febd5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fashion-MNIST with normalization\n",
    "mean, std = (0.2860,), (0.3530,)\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "fashion_train = datasets.FashionMNIST(root='data', train=True, download=True, transform=base_transform)\n",
    "fashion_test = datasets.FashionMNIST(root='data', train=False, download=True, transform=base_transform)\n",
    "\n",
    "print(fashion_train, fashion_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6b095b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "          'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e47d1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few examples (unnormalized for visualization)\n",
    "fig, axs = plt.subplots(3, 5, figsize=(9, 7))\n",
    "for i, ax in enumerate(axs.ravel()):\n",
    "    image, label = fashion_train[i]\n",
    "    image = image * std[0] + mean[0]\n",
    "    ax.imshow(image.squeeze(), cmap='gray')\n",
    "    ax.set_title(labels[label], fontsize=8)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3f504c",
   "metadata": {},
   "source": [
    "We will use PyTorch `Subset` objects to create train/validation/test splits without writing images to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acab26f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "# Reduce dataset sizes for faster experimentation\n",
    "generator = torch.Generator().manual_seed(109)\n",
    "train_size = 1200\n",
    "val_size = 240\n",
    "test_size = 1000\n",
    "\n",
    "train_val_indices = torch.randperm(len(fashion_train), generator=generator)[:train_size + val_size]\n",
    "train_indices = train_val_indices[:train_size].tolist()\n",
    "val_indices = train_val_indices[train_size:].tolist()\n",
    "test_indices = torch.randperm(len(fashion_test), generator=generator)[:test_size].tolist()\n",
    "\n",
    "train_subset = Subset(fashion_train, train_indices)\n",
    "val_subset = Subset(fashion_train, val_indices)\n",
    "test_subset = Subset(fashion_test, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cedea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "data_batch, labels_batch = next(iter(train_loader))\n",
    "print('data batch shape:', data_batch.shape)\n",
    "print('labels batch shape:', labels_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce59b803",
   "metadata": {},
   "source": [
    "> **‚ùì Question 4: Subset Splits**\n",
    ">\n",
    "> 1. Why do we create separate train/validation/test subsets instead of reusing the full dataset?\n",
    "> 2. How could class imbalance in a small subset affect validation accuracy?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a628b063",
   "metadata": {},
   "source": [
    "### Build a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a204849b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMLP(nn.Module):\n",
    "    def __init__(self, input_dim=28*28, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tkyheg4kkrp",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model, train_loader, val_loader, optimizer, criterion, epochs=30, early_stopping=None):\n",
    "    history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(xb)\n",
    "            loss = criterion(outputs, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * xb.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_acc = correct / total\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                outputs = model(xb)\n",
    "                loss = criterion(outputs, yb)\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                val_correct += (preds == yb).sum().item()\n",
    "                val_total += yb.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        history['loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['accuracy'].append(train_acc)\n",
    "        history['val_accuracy'].append(val_acc)\n",
    "\n",
    "        if early_stopping and early_stopping.step(val_loss, model):\n",
    "            break\n",
    "\n",
    "    if early_stopping:\n",
    "        early_stopping.restore(model)\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_classifier(model, loader, criterion):\n",
    "    model.eval()\n",
    "    loss_total = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            outputs = model(xb)\n",
    "            loss = criterion(outputs, yb)\n",
    "            loss_total += loss.item() * xb.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "    return loss_total / len(loader.dataset), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a90b135",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(109)\n",
    "model_cls = FashionMLP().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_cls.parameters(), lr=0.001)\n",
    "\n",
    "history_cls = train_classifier(\n",
    "    model_cls,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    epochs=30,\n",
    "    early_stopping=EarlyStopping(patience=5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdad8d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc = evaluate_classifier(model_cls, train_loader, criterion)\n",
    "val_loss, val_acc = evaluate_classifier(model_cls, val_loader, criterion)\n",
    "test_loss, test_acc = evaluate_classifier(model_cls, test_loader, criterion)\n",
    "\n",
    "print(f\"Train ‚Äî Loss: {train_loss:.4f}, Acc: {train_acc:.2f}\")\n",
    "print(f\"Val   ‚Äî Loss: {val_loss:.4f}, Acc: {val_acc:.2f}\")\n",
    "print(f\"Test  ‚Äî Loss: {test_loss:.4f}, Acc: {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb8a48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "print(f\"Val Acc at last training epoch (before best-model restore): {history_cls['val_accuracy'][-1]:.2f}\")   \n",
    "axs[0].plot(history_cls['accuracy'])\n",
    "axs[0].plot(history_cls['val_accuracy'])\n",
    "axs[0].set_title('Baseline Classifier ‚Äî Accuracy')\n",
    "axs[0].set_ylabel('accuracy')\n",
    "axs[0].set_xlabel('epoch')\n",
    "axs[0].grid(True, alpha=0.3)\n",
    "axs[0].legend(['train', 'validation'], loc='upper left')\n",
    "\n",
    "axs[1].plot(history_cls['loss'])\n",
    "axs[1].plot(history_cls['val_loss'])\n",
    "axs[1].set_title('Baseline Classifier ‚Äî Loss')\n",
    "axs[1].set_ylabel('loss')\n",
    "axs[1].set_xlabel('epoch')\n",
    "axs[1].grid(True, alpha=0.3)\n",
    "axs[1].legend(['train', 'validation'], loc='upper left')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c6085d",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "In Part 1, we added Gaussian noise directly inside the model as a regularizer. For image data, we can go further ‚Äî applying transformations that exploit known invariances. A T-shirt is still a T-shirt whether flipped horizontally or slightly rotated. By applying random transforms on-the-fly, the model sees a different variation of each image every epoch, even though the dataset itself remains 1,200 samples. Below we combine random flips, small rotations, and Gaussian noise as a `torchvision` transform pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7fafac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddGaussianNoise:\n",
    "    def __init__(self, std=0.05):\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return torch.clamp(tensor + torch.randn_like(tensor) * self.std, 0.0, 1.0)\n",
    "\n",
    "augment_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  #Default probability is 0.5\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    AddGaussianNoise(0.05),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "fashion_train_aug = datasets.FashionMNIST(root='data', train=True, download=False, transform=augment_transform)\n",
    "train_subset_aug = Subset(fashion_train_aug, train_indices)\n",
    "train_loader_aug = torch.utils.data.DataLoader(train_subset_aug, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w547xxqmde",
   "metadata": {},
   "source": [
    "> **‚ùì Question 5: Choosing Augmentations**\n",
    ">\n",
    "> Not all augmentations are appropriate for every dataset. Consider the transforms we used: `RandomHorizontalFlip`, `RandomRotation(10)`, and `AddGaussianNoise`.\n",
    ">\n",
    "> 1. Why is `RandomHorizontalFlip` a reasonable augmentation for Fashion-MNIST? For which classes (if any) might it *not* preserve the label?\n",
    "> 2. Why do we limit rotation to just 10 degrees? What could go wrong with larger rotations (e.g., 90¬∞)?\n",
    "> 3. We did **not** include `RandomVerticalFlip`. Why would vertical flips be a bad augmentation for clothing images?\n",
    "> 4. If you were working with a medical imaging dataset (e.g., chest X-rays), which of these augmentations would still be appropriate and which would not?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406cb7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(109)\n",
    "model_cls_aug = FashionMLP().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_cls_aug.parameters(), lr=0.001)\n",
    "\n",
    "history_aug = train_classifier(\n",
    "    model_cls_aug,\n",
    "    train_loader_aug,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    epochs=60,\n",
    "    early_stopping=EarlyStopping(patience=10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a179bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "print(f\"Final epoch val Acc: {history_aug['val_accuracy'][-1]:.2f}\")\n",
    "print(f\"Best epoch val Acc: {max(history_aug['val_accuracy']):.2f}\")\n",
    "\n",
    "axs[0].plot(history_aug['accuracy'])\n",
    "axs[0].plot(history_aug['val_accuracy'])\n",
    "axs[0].set_title('Augmented Classifier ‚Äî Accuracy')\n",
    "axs[0].set_ylabel('accuracy')\n",
    "axs[0].set_xlabel('epoch')\n",
    "axs[0].legend(['train', 'validation'], loc='upper left')\n",
    "\n",
    "axs[1].plot(history_aug['loss'])\n",
    "axs[1].plot(history_aug['val_loss'])\n",
    "axs[1].set_title('Augmented Classifier ‚Äî Loss')\n",
    "axs[1].set_ylabel('loss')\n",
    "axs[1].set_xlabel('epoch')\n",
    "axs[1].legend(['train', 'validation'], loc='upper left')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6cb9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss, train_acc = evaluate_classifier(model_cls_aug, train_loader, criterion)\n",
    "val_loss, val_acc = evaluate_classifier(model_cls_aug, val_loader, criterion)\n",
    "test_loss, test_acc = evaluate_classifier(model_cls_aug, test_loader, criterion)\n",
    "\n",
    "print(f\"Train ‚Äî Loss: {train_loss:.4f}, Acc: {train_acc:.2f}\")\n",
    "print(f\"Val   ‚Äî Loss: {val_loss:.4f}, Acc: {val_acc:.2f}\")\n",
    "print(f\"Test  ‚Äî Loss: {test_loss:.4f}, Acc: {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3l0pylapqvk",
   "metadata": {},
   "source": [
    "**Results comparison:** Augmentation improved validation accuracy over the baseline on our small 1,200-sample training set, while test accuracy remained similar. With only 1,000 test samples, a few points of difference can be noise, and the validation split comes from the same `fashion_train` pool, so gains may not transfer to the separate test set. Always rely on a truly held-out test set and avoid over-interpreting small differences.\n",
    "\n",
    "Note: exact metrics can vary across runs and hardware (CPU vs GPU) due to nondeterminism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ekruz59e9t",
   "metadata": {},
   "source": [
    "> **‚ùì Question 6: Model Layer vs. Data Transform for Noise**\n",
    ">\n",
    "> In Part 1 we added Gaussian noise as a **model layer** (`GaussianNoise(nn.Module)`), while in Part 2 we added it as a **data transform** (`AddGaussianNoise` in `transforms.Compose`). Both inject random noise during training to regularize the model.\n",
    ">\n",
    "> 1. At what point in the pipeline does noise get applied in each approach? How does this affect what the noise \"means\" (pixel space vs. normalized space)?\n",
    "> 2. What are the trade-offs of each approach? When might you prefer one over the other?\n",
    "> 3. The `AddGaussianNoise` transform clamps values to [0, 1]. Why is this necessary here but not in the `GaussianNoise` module?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11454599",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üèãÔ∏è TEAM ACTIVITY: Fashion-MNIST Regularization\n",
    "\n",
    "Try improving validation accuracy beyond 83% without changing the overall network depth. Consider tuning dropout rates, batch normalization placement, optimizers, and augmentation settings. Track your best configuration.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tliqlh02fwb",
   "metadata": {},
   "source": [
    "**Define an enhanced model with dropout**\n",
    "\n",
    "Create a modified `FashionMLP` that adds dropout layers between the hidden layers. Keep the same layer sizes (256 ‚Üí 128 ‚Üí 64 ‚Üí 32) but add `nn.Dropout` after the first two ReLU activations.\n",
    "\n",
    "**Hint:** A dropout rate of 0.2 provides regularization without over-constraining the model. Don't add dropout to every layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uonov4rjgcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ylvm0wwmbm",
   "metadata": {},
   "source": [
    "**Create augmentation with small translations**\n",
    "\n",
    "The baseline augmentation already includes flips, rotation, and noise. Add small random translations via `RandomAffine` to simulate slightly shifted images.\n",
    "\n",
    "**Hint:** Keep translations small (5% of image size) to avoid shifting important features out of frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cb1vd8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uj2lg8rk0c8",
   "metadata": {},
   "source": [
    "**Train and evaluate**\n",
    "\n",
    "Train your regularized model. The dropout already provides regularization, so we don't need heavy weight decay.\n",
    "\n",
    "**Hint:** Standard Adam with `lr=0.001` works well with dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0jyi4ux0c6ui",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lhicmg9g3uq",
   "metadata": {},
   "source": [
    "**Plot training curves**\n",
    "\n",
    "Plot the accuracy and loss curves for your regularized model. Compare the gap between training and validation ‚Äî is it smaller than the baseline?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i0id34z0o4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ybghy4wfwm",
   "metadata": {},
   "source": [
    "**End of team activity**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
