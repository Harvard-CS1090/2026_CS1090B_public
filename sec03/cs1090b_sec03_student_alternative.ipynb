{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4633457e",
   "metadata": {
    "id": "4633457e"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Harvard-CS1090/2026_CS1090B_public/blob/main/sec03/cs1090b_sec03_student_alternative.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yexkuYSQxZba",
   "metadata": {
    "id": "yexkuYSQxZba"
   },
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\">\n",
    "\n",
    "# CS1090B Section 3 (alternative): Regularization and Data Augmentation\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Spring 2026**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Kevin Rader, and Chris Gumb<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pp3se5aelvl",
   "metadata": {
    "id": "pp3se5aelvl"
   },
   "source": [
    "## Overview\n",
    "\n",
    "In this section, we investigate a central question in deep learning:\n",
    "\n",
    "> **Why do neural networks overfit, and how can we systematically improve generalization?**\n",
    "\n",
    "We approach this question in two stages.\n",
    "\n",
    "---\n",
    "\n",
    "### Part 1: Understanding Regularization (Regression Setting)\n",
    "\n",
    "We begin with a controlled synthetic regression problem where overfitting is easy to observe and diagnose. In this setting, we:\n",
    "\n",
    "- Visualize how overfitting emerges in training vs. validation loss curves  \n",
    "- Examine how model capacity affects generalization  \n",
    "- Compare four regularization strategies:\n",
    "  - Early stopping  \n",
    "  - L1 and L2 weight penalties  \n",
    "  - Dropout  \n",
    "  - Gaussian noise augmentation  \n",
    "\n",
    "The goal is not just to apply these techniques, but to understand **how each one constrains the model** and why that improves generalization.\n",
    "\n",
    "---\n",
    "\n",
    "### Part 2: Regularization in Practice (Image Classification)\n",
    "\n",
    "We then move to a more realistic setting: image classification on Fashion-MNIST.\n",
    "\n",
    "Here, we:\n",
    "\n",
    "- Train a neural network under limited data conditions  \n",
    "- Observe overfitting in a high-dimensional input space  \n",
    "- Apply image-based data augmentation (flips, rotations, noise)  \n",
    "\n",
    "This allows us to see how augmentation acts as an *implicit regularizer* by modifying the effective training distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ALHj2sv07noA",
   "metadata": {
    "id": "ALHj2sv07noA"
   },
   "source": [
    "## Learning Objectives\n",
    "\n",
    "By the end of this section, you should be able to:\n",
    "\n",
    "### Conceptual Understanding\n",
    "1. Define overfitting and explain how it appears in training vs. validation loss curves.\n",
    "2. Explain why regularization improves generalization.\n",
    "3. Compare different regularization techniques and describe how they constrain model capacity.\n",
    "4. Understand data augmentation as implicit regularization.\n",
    "\n",
    "### Practical Skills (PyTorch + Modeling)\n",
    "5. Implement and train a neural network in PyTorch for regression and classification tasks.\n",
    "6. Diagnose overfitting using learning curves.\n",
    "7. Apply and compare the following regularization methods:\n",
    "   - L1 / L2 weight penalties\n",
    "   - Early stopping\n",
    "   - Dropout\n",
    "   - Gaussian noise augmentation\n",
    "8. Modify optimizers to include weight decay.\n",
    "9. Use dropout correctly during training and inference.\n",
    "10. Implement data augmentation for image classification using torchvision transforms.\n",
    "11. Using PyTorch DataLoader to implement SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16938ed3",
   "metadata": {},
   "source": [
    "## Setup: Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f50eb5",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "99f50eb5",
    "outputId": "ab51cab4-2ac6-4c39-ceae-70c3236a5db7"
   },
   "outputs": [],
   "source": [
    "#@title Colab Setup\n",
    "# Environment detection and setup\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "# Define the zip file URL and expected directories\n",
    "assets_zip_url = \"https://github.com/Harvard-CS1090/2026_CS1090B_public/raw/main/sec03/notebook_assets.zip\"\n",
    "\n",
    "assets_zip_name = \"notebook_assets.zip\"\n",
    "expected_dirs = [\"data\", \"fig\"]\n",
    "\n",
    "# Check if required directories already exist\n",
    "all_dirs_exist = all(os.path.isdir(d) for d in expected_dirs)\n",
    "\n",
    "if all_dirs_exist:\n",
    "    print(\"Required directories already exist. Skipping download.\")\n",
    "else:\n",
    "    print(f\"Downloading {assets_zip_name} from GitHub...\")\n",
    "\n",
    "    # Use wget in Colab, or urllib for local\n",
    "    try:\n",
    "        if 'google.colab' in sys.modules:\n",
    "            subprocess.run(['wget', '-q', assets_zip_url], check=True)\n",
    "        else:\n",
    "            import urllib.request\n",
    "            urllib.request.urlretrieve(assets_zip_url, assets_zip_name)\n",
    "        print(f\"Downloaded {assets_zip_name}.\")\n",
    "\n",
    "        # Unzip the file\n",
    "        import zipfile\n",
    "        with zipfile.ZipFile(assets_zip_name, 'r') as zip_ref:\n",
    "            zip_ref.extractall('.')\n",
    "        print(f\"Extracted {assets_zip_name}.\")\n",
    "\n",
    "        # Clean up the zip file\n",
    "        os.remove(assets_zip_name)\n",
    "        print(f\"Removed {assets_zip_name}.\")\n",
    "\n",
    "        # Remove __MACOSX folder if it exists\n",
    "        if os.path.isdir('__MACOSX'):\n",
    "            shutil.rmtree('__MACOSX')\n",
    "            print(\"Removed __MACOSX folder.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during setup: {e}\", file=sys.stderr)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d3a337",
   "metadata": {
    "id": "d0d3a337"
   },
   "source": [
    "## Part 1: Regularization for Regression\n",
    "\n",
    "When a model fits the training data too closely - capturing noise rather than the underlying pattern - it **overfits**, performing well on training examples but poorly on unseen data. Regularization techniques address this by constraining the model in various ways.\n",
    "\n",
    "In this section, we'll intentionally overfit a neural network on a regression task, then apply different regularization techniques to combat it: early stopping, L1/L2 weight penalties, dropout, and data augmentation via Gaussian noise. We'll compare how each method affects the training and validation loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffba45a3",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ffba45a3",
    "outputId": "c612f8a1-14c9-4ef1-97b5-01f7cc63b7dc"
   },
   "outputs": [],
   "source": [
    "#@title Imports, Device Setup, and Random Seeds\n",
    "import copy\n",
    "import os\n",
    "import random as rn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "\n",
    "# Reproducibility\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(109)\n",
    "rn.seed(109)\n",
    "torch.manual_seed(109);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03jkafac38ku",
   "metadata": {
    "id": "03jkafac38ku"
   },
   "source": [
    "**Dataset:** We are using a synthetic dataset based on the function  \n",
    "$$y = x \\sin(x)$$\n",
    "\n",
    "with added Gaussian noise.  \n",
    "\n",
    "The exact functional form is not the focus here. What matters is that the relationship between input and output is **nonlinear** and **noisy**, which makes it an ideal setting for observing overfitting and testing regularization strategies.\n",
    "\n",
    "Because the signal is structured but imperfect, the model can either:\n",
    "- Learn the underlying pattern, or  \n",
    "- Start memorizing noise  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d04b914",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d04b914",
    "outputId": "773eb08e-140d-47dc-e7bb-db04316e2517"
   },
   "outputs": [],
   "source": [
    "#@title Generating a (Noisy) Toy Dataset: $f(x)= x\\text{sin}(x)$\n",
    "# We'll model noisy data from the function f(x) = x * sin(x)\n",
    "def f(x):\n",
    "    return x * np.sin(x)\n",
    "\n",
    "N = 25\n",
    "x = np.linspace(0, 5, N) # small, equally spaced data points for simplicity\n",
    "y = f(x) + np.random.normal(0, 0.5, len(x)) # noisy data\n",
    "df = pd.DataFrame({'x': x, 'y': y})\n",
    "\n",
    "# Split to train and val (no test for our examples)\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    df['x'],\n",
    "    df['y'],\n",
    "    train_size=0.7,\n",
    "    random_state=109\n",
    ")\n",
    "\n",
    "# We'll use standardized data for input to the NN\n",
    "scaler = StandardScaler()\n",
    "x_train_std = scaler.fit_transform(x_train.to_frame()).ravel()\n",
    "x_val_std = scaler.transform(x_val.to_frame()).ravel()\n",
    "print(f\"Generated {N} data points!\")\n",
    "\n",
    "# Torch models will want torch tensor objects\n",
    "x_train_t = torch.tensor(x_train_std, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "y_train_t = torch.tensor(y_train.values, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "x_val_t = torch.tensor(x_val_std, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "y_val_t = torch.tensor(y_val.values, dtype=torch.float32).unsqueeze(1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944ba2ad",
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "944ba2ad",
    "outputId": "25b514fb-c783-46b5-8874-cbad457130c8"
   },
   "outputs": [],
   "source": [
    "#@title Visualizing the Data\n",
    "x_lin = np.linspace(0, 5, 1000) # large linspace for plotting\n",
    "\n",
    "def plot_data_with_true_fn(df, idx, ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    train_mask = np.ones(df.shape[0], dtype=bool)\n",
    "    train_mask[idx] = False\n",
    "    ax.scatter(df.x[train_mask], df.y[train_mask], c='b', label='train data')\n",
    "    ax.scatter(df.x[~train_mask], df.y[~train_mask], c='orange', marker='^', label='validation data')\n",
    "    ax.plot(x_lin, f(x_lin), label=\"true function\", alpha=0.5, c='k')\n",
    "    ax.set_xlabel(r'$x$')\n",
    "    ax.set_ylabel(r'$\\hat{y}$')\n",
    "    ax.legend()\n",
    "\n",
    "plot_data_with_true_fn(df, idx=x_val.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71f2111",
   "metadata": {
    "id": "a71f2111"
   },
   "source": [
    "We will start with a small neural network and intentionally overfit. Below we define a baseline model using `nn.Sequential` and reusable plotting utilities that we'll use throughout Part 1 to compare different regularization strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dYgJ_gGxUVmY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dYgJ_gGxUVmY",
    "outputId": "bc1649b7-c776-4b71-db18-0c45235338c0"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(109)\n",
    "model1 = nn.Sequential(\n",
    "            nn.Linear(1, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 1)\n",
    "        ).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model1.parameters(), lr=0.0003)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epochs = 3000\n",
    "history1 = {'loss': [], 'val_loss': []}\n",
    "for epoch in range(epochs):\n",
    "    model1.train()\n",
    "    optimizer.zero_grad()\n",
    "    preds_train = model1(x_train_t)\n",
    "    loss = criterion(preds_train, y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss = loss.item()\n",
    "\n",
    "    model1.eval()\n",
    "    with torch.no_grad():\n",
    "        preds_val = model1(x_val_t)\n",
    "        loss = criterion(preds_val, y_val_t)\n",
    "        val_loss = loss.item()\n",
    "\n",
    "    history1['loss'].append(train_loss)\n",
    "    history1['val_loss'].append(val_loss)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch:4d}: train loss = {train_loss:.4f}, \\\n",
    "              val loss = {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BgrX6AZS-chs",
   "metadata": {
    "id": "BgrX6AZS-chs"
   },
   "source": [
    "> **‚ùì Question 1: Understanding the Training Loop**\n",
    ">\n",
    "> Examine the training code above and explain what each of the following does:\n",
    ">\n",
    "> 1. `optimizer = optim.Adam(model1.parameters(), lr=0.0003)`\n",
    "> 2. `criterion = nn.MSELoss()`\n",
    "> 3. `loss.backward()` and `optimizer.step()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fi4tri1tqal",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "id": "fi4tri1tqal",
    "outputId": "8e099e14-ae20-4018-ac75-355c23a91f51"
   },
   "outputs": [],
   "source": [
    "#@title Initial Training Results\n",
    "\n",
    "# Some helper functions we'll use throughout for plot training results\n",
    "def plot_history(history, title=None, ax=None, best_val=False):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    ax.plot(history['loss'], label='train')\n",
    "    ax.plot(history['val_loss'], label='validation')\n",
    "\n",
    "    ax.set_yscale('log')  # <-- log scale for MSE\n",
    "\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.set_ylabel('MSE (log scale)')\n",
    "\n",
    "    best_loss = np.nanmin(history['val_loss'])\n",
    "    best_epoch = np.nanargmin(history['val_loss'])\n",
    "\n",
    "    if best_val:\n",
    "        ax.axvline(best_epoch, c='k', ls='--',\n",
    "                label=f'best val loss = {best_loss:.2e}')\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    else:\n",
    "        ax.set_title(\"Training History\")\n",
    "\n",
    "def plot_predictions(model, ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    model.eval()\n",
    "    x_lin = np.linspace(df.x.min(), df.x.max(), 500).reshape(-1, 1)\n",
    "    x_lin_std = scaler.transform(pd.DataFrame(x_lin, columns=['x']))\n",
    "    with torch.no_grad():\n",
    "        x_tensor = torch.tensor(x_lin_std, dtype=torch.float32).to(device)\n",
    "        preds = model(x_tensor).detach().cpu().numpy()\n",
    "        preds_val = model(x_val_t)\n",
    "    ax.plot(x_lin, preds, c='blue', alpha=0.8, label='prediction')\n",
    "    ax.legend()\n",
    "    # Here we reference criterion which is out of function scope.\n",
    "    # This is bad practice but it makes the function calls simpler\n",
    "    # and we never change the definition of criterion throughout\n",
    "    # part 1.\n",
    "    final_val_loss = criterion(preds_val, y_val_t).item()\n",
    "    ax.set_title(f\"Final Model\\n(val loss = {final_val_loss:.4f})\")\n",
    "\n",
    "def plot_data(df, idx=None, ax=None):\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    # If validation indices are provided, separate train and val\n",
    "    if idx is not None:\n",
    "        train_df = df.drop(idx)\n",
    "        val_df = df.loc[idx]\n",
    "\n",
    "        ax.scatter(train_df['x'], train_df['y'],\n",
    "                   color='gray', alpha=0.6, label='train data')\n",
    "        ax.scatter(val_df['x'], val_df['y'],\n",
    "                   color='red', alpha=0.8, label='validation data')\n",
    "    else:\n",
    "        ax.scatter(df['x'], df['y'],\n",
    "                   color='gray', alpha=0.6, label='data')\n",
    "\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.legend()\n",
    "\n",
    "def plot_results(model, history, title=\"Model Performance\", best_val=False):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "    plot_history(history, ax=axs[0], best_val=best_val)\n",
    "    plot_data(df, idx=x_val.index, ax=axs[1])\n",
    "    plot_predictions(model, ax=axs[1])\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(title, fontsize=16, y=1.05)\n",
    "    plt.show()\n",
    "\n",
    "plot_results(model1, history1, \"Baseline (no regularization)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6123f4a2",
   "metadata": {
    "id": "6123f4a2"
   },
   "source": [
    "> **‚ùì Question 2: Diagnosing Overfitting**\n",
    ">\n",
    "> 1. What do you notice about the gap between training and validation loss as epochs increase?\n",
    "> 2. Why does the validation loss start to increase even as training loss continues to decrease?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50519748",
   "metadata": {
    "id": "50519748"
   },
   "source": [
    "### Weight Decay (L2) and L1 Regularization\n",
    "\n",
    "Weight penalties add a cost to having large weights, discouraging the model from fitting noise. **L2 regularization** (weight decay) adds $\\lambda \\sum w^2$ to the loss, which shrinks all weights toward zero but rarely makes them exactly zero. **L1 regularization** adds $\\lambda \\sum |w|$ to the loss, which encourages *sparsity* - pushing many weights to exactly zero while keeping a few large ones. In practice, L2 produces smoother models while L1 can act as a form of feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5uwzb3qxufw",
   "metadata": {
    "id": "5uwzb3qxufw"
   },
   "source": [
    "**Note:** PyTorch also provides `torch.optim.AdamW`, which applies weight decay *directly* to the weights rather than through the gradient. In standard `Adam`, the penalty gets scaled down by the adaptive learning rate, weakening it for frequently-updated parameters. `AdamW` avoids this and is preferred in modern practice (e.g., transformer fine-tuning). For this small example the difference is negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jiPWY_oOe6Vz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "id": "jiPWY_oOe6Vz",
    "outputId": "724a49b2-5b48-4ac0-a48a-39dabe09f3af"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(109)\n",
    "model2 = nn.Sequential(\n",
    "            nn.Linear(1, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 1)\n",
    "        ).to(device)\n",
    "\n",
    "# NOTE: For our optimizer we use AdamW and a new argument, `weight_decay`\n",
    "PENALTY = 0.5\n",
    "optimizer = optim.AdamW(model2.parameters(), lr=0.0003, weight_decay=PENALTY)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epochs = 3000\n",
    "history2 = {'loss': [], 'val_loss': []}\n",
    "for epoch in range(epochs):\n",
    "    model2.train()\n",
    "    optimizer.zero_grad()\n",
    "    preds_train = model2(x_train_t)\n",
    "    loss = criterion(preds_train, y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss = loss.item()\n",
    "\n",
    "    model2.eval()\n",
    "    with torch.no_grad():\n",
    "        preds_val = model2(x_val_t)\n",
    "        loss = criterion(preds_val, y_val_t)\n",
    "        val_loss = loss.item()\n",
    "\n",
    "    history2['loss'].append(train_loss)\n",
    "    history2['val_loss'].append(val_loss)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch:4d}: train loss = {train_loss:.4f}, \\\n",
    "              val loss = {val_loss:.4f}\")\n",
    "\n",
    "plot_results(model2, history2, \"L2 Weight Decay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LEvM2uDnDSWR",
   "metadata": {
    "id": "LEvM2uDnDSWR"
   },
   "source": [
    "> **‚ùì Question 3: Regularization Strength**\n",
    "> 1. Is this enough regularization? Try different values.\n",
    "> 2. What do you expect to happen if `weight_decay` becomes very large? Think about: what happens to model capacity, what kind of functions the network can represent, and what happens to training and validation loss.\n",
    "> 3. If too little regularization leads to overfitting and too much leads to underfitting, how do we choose the right `weight_decay`?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W-DEzXg0D3zJ",
   "metadata": {
    "id": "W-DEzXg0D3zJ"
   },
   "source": [
    "---\n",
    "\n",
    "üìù **Note on L1 Regularization**\n",
    "\n",
    "Unlike L2 (which can be added directly using `weight_decay` in optimizers like `AdamW`),  \n",
    "there is **no built-in simple argument for L1 regularization** in PyTorch optimizers.\n",
    "\n",
    "Instead, we implement L1 manually by adding an extra penalty term to the loss:\n",
    "\n",
    "$$\n",
    "\\text{loss} = \\text{original loss} + \\lambda \\sum |w|\n",
    "$$\n",
    "\n",
    "In practice, this means:\n",
    "\n",
    "- Compute the standard loss (e.g., MSE).\n",
    "- Compute the L1 penalty by summing the absolute values of the weights.\n",
    "- Add the penalty to the loss before calling `backward()`.\n",
    "\n",
    "Here is a minimal reference implementation:\n",
    "\n",
    "```python\n",
    "\n",
    "l1_lambda = 0.005\n",
    "# Inside the training loop, after computing the standard loss:\n",
    "loss = criterion(preds, targets)\n",
    "# Add L1 penalty on weights only (not biases ‚Äî standard convention)\n",
    "l1_penalty = sum(p.abs().sum() for name, p in model.named_parameters() if 'weight' in name)\n",
    "loss = loss + l1_lambda * l1_penalty\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77816e8d",
   "metadata": {
    "id": "77816e8d"
   },
   "source": [
    "### Early Stopping\n",
    "\n",
    "Early stopping monitors validation loss during training and halts the process when performance stops improving. The idea is simple: as training continues, the model begins to memorize noise in the training data, causing validation loss to rise even as training loss keeps falling. By saving the best model and stopping once validation loss hasn't improved for a set number of epochs (*patience*), we get a model that generalizes better without needing to tune a regularization hyperparameter directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RHPCE1DQi407",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "id": "RHPCE1DQi407",
    "outputId": "20c26234-db34-461b-a678-18abd382110a"
   },
   "outputs": [],
   "source": [
    "# Model with Early Stopping\n",
    "torch.manual_seed(109)\n",
    "model3 = nn.Sequential(\n",
    "            nn.Linear(1, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 1)\n",
    "        ).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model3.parameters(), lr=0.0003)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epochs = 3000 # Max epochs to run if not stopped early\n",
    "history3 = {'loss': [], 'val_loss': []}\n",
    "\n",
    "# Early Stopping parameters\n",
    "patience = 200\n",
    "best_val_loss = np.inf\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"Training model with Early Stopping (direct implementation)...\")\n",
    "for epoch in range(epochs):\n",
    "    model3.train()\n",
    "    optimizer.zero_grad()\n",
    "    preds_train = model3(x_train_t)\n",
    "    loss = criterion(preds_train, y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss = loss.item()\n",
    "\n",
    "    model3.eval()\n",
    "    with torch.no_grad():\n",
    "        preds_val = model3(x_val_t)\n",
    "        loss = criterion(preds_val, y_val_t)\n",
    "        val_loss = loss.item()\n",
    "\n",
    "    history3['loss'].append(train_loss)\n",
    "    history3['val_loss'].append(val_loss)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch:4d}: train loss = {train_loss:.4f}, val loss = {val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered at epoch {epoch}. Validation loss did not improve for {patience} epochs.\")\n",
    "        break\n",
    "\n",
    "# Plotting results for the early stopped model\n",
    "plot_results(model3, history3, \"Early Stopping /w Patience\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7nVA2MfEt5R",
   "metadata": {
    "id": "b7nVA2MfEt5R"
   },
   "source": [
    "> **‚ùì Question 4: Understanding Early Stopping**\n",
    ">\n",
    "> Read the training loop carefully and think about the following:\n",
    ">\n",
    "> 1. What is the role of `best_val_loss`? Why is it initialized to `np.inf`? What does it track during training?\n",
    "> 2. What does `patience = 200` mean? What happens if patience is very small? What happens if patience is very large?\n",
    "> 3. Why do we reset `patience_counter` when validation loss improves?\n",
    "> 4. Why do we monitor **validation loss** and not training loss? What would happen if we stopped based on training loss instead?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9Lkr1AdFFvEt",
   "metadata": {
    "id": "9Lkr1AdFFvEt"
   },
   "source": [
    "> **‚ùì Question 5: Best Model Restoration**\n",
    ">\n",
    "> When early stopping is triggered, we stop training ‚Äî but are we using the model from the **last epoch before stopping**, or the model that achieved the **lowest validation loss**? Are these always the same? What is best practice?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wL8EVyGwkCzs",
   "metadata": {
    "id": "wL8EVyGwkCzs"
   },
   "source": [
    "### Restoring Our Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "M6bulA9lkBZ7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "id": "M6bulA9lkBZ7",
    "outputId": "ccaf0c4b-b41b-4a0b-c9e2-1e8609b50a50"
   },
   "outputs": [],
   "source": [
    "# Model with Early Stopping\n",
    "torch.manual_seed(109)\n",
    "model4 = nn.Sequential(\n",
    "            nn.Linear(1, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 1)\n",
    "        ).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model4.parameters(), lr=0.0003)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epochs = 3000 # Max epochs to run if not stopped early\n",
    "history4 = {'loss': [], 'val_loss': []}\n",
    "\n",
    "# Early Stopping parameters\n",
    "patience = 100\n",
    "best_val_loss = np.inf\n",
    "patience_counter = 0\n",
    "best_model_state = None\n",
    "\n",
    "print(\"Training model with Early Stopping (direct implementation)...\")\n",
    "for epoch in range(epochs):\n",
    "    model4.train()\n",
    "    optimizer.zero_grad()\n",
    "    preds_train = model4(x_train_t)\n",
    "    loss = criterion(preds_train, y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss = loss.item()\n",
    "\n",
    "    model4.eval()\n",
    "    with torch.no_grad():\n",
    "        preds_val = model4(x_val_t)\n",
    "        loss = criterion(preds_val, y_val_t)\n",
    "        val_loss = loss.item()\n",
    "\n",
    "    history4['loss'].append(train_loss)\n",
    "    history4['val_loss'].append(val_loss)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch:4d}: train loss = {train_loss:.4f}, val loss = {val_loss:.4f}\")\n",
    "\n",
    "    # Early stopping logic\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        best_model_state = copy.deepcopy(model4.state_dict()) # Save the best model state\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping triggered at epoch {epoch}. Validation loss did not improve for {patience} epochs.\")\n",
    "        break\n",
    "\n",
    "# Load the best model weights if early stopping occurred and a state was saved\n",
    "if best_model_state is not None:\n",
    "    model4.load_state_dict(best_model_state)\n",
    "    print(\"Restored model to best state.\")\n",
    "else:\n",
    "    print(\"No best model state to restore (perhaps val_loss never improved).\")\n",
    "\n",
    "# Plotting results for the early stopped model\n",
    "plot_results(model4, history4, \"Early Stopping /w Patience & Best Weight Restoring\", best_val=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad16ad96",
   "metadata": {},
   "source": [
    "üí¨ **Discuss:**\n",
    "\n",
    "‚ÅâÔ∏è **Q:** How is the weight restoration implemented? What do `state_dict()` and `load_state_dict()` do?        \n",
    "\n",
    "‚ÅâÔ∏è **Q:** Why do we need `copy.deepcopy` here instead of a simple assignment?\n",
    "\n",
    "‚ÅâÔ∏è **Q:** How can we improve our implementation of early stopping? (Hint: think about minimum improvement thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62012d85",
   "metadata": {
    "id": "62012d85"
   },
   "source": [
    "### Dropout\n",
    "\n",
    "Dropout randomly zeroes a fraction of neuron activations during each training step, forcing the network to not rely on any single neuron. This acts as an implicit ensemble ‚Äî on each forward pass, a different \"thinned\" subnetwork is active, and the final model approximates an average over all these subnetworks. At evaluation time, dropout is turned off and all neurons are active (with outputs scaled accordingly). The `dropout` parameter controls what fraction of activations are dropped (e.g., 0.3 means 30% are zeroed)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc765cd0",
   "metadata": {
    "id": "bc765cd0"
   },
   "source": [
    "**From Lecture 4:** Dropout randomly \"drops\" neurons during training, forcing the network to learn redundant representations and reducing co-adaptation between neurons.\n",
    "\n",
    "<img src=./fig/lec04_dropout_mechanism.png width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a83dee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 692
    },
    "id": "e3a83dee",
    "outputId": "0ef27cad-1a61-4f1d-e7bc-d00da2d030ca"
   },
   "outputs": [],
   "source": [
    "dropout_p = 0.1\n",
    "torch.manual_seed(109)\n",
    "model5 = nn.Sequential(\n",
    "            nn.Linear(1, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(100, 1)\n",
    "        ).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model5.parameters(), lr=0.0003)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epochs = 3000\n",
    "history5 = {'loss': [], 'val_loss': []}\n",
    "for epoch in range(epochs):\n",
    "    model5.train()\n",
    "    optimizer.zero_grad()\n",
    "    preds_train = model5(x_train_t)\n",
    "    loss = criterion(preds_train, y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss = loss.item()\n",
    "\n",
    "    model5.eval()\n",
    "    with torch.no_grad():\n",
    "        preds_val = model5(x_val_t)\n",
    "        loss = criterion(preds_val, y_val_t)\n",
    "        val_loss = loss.item()\n",
    "\n",
    "    history5['loss'].append(train_loss)\n",
    "    history5['val_loss'].append(val_loss)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch:4d}: train loss = {train_loss:.4f}, \\\n",
    "              val loss = {val_loss:.4f}\")\n",
    "\n",
    "plot_results(model5, history5, \"Dropout\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86659b08",
   "metadata": {},
   "source": [
    "üí¨ **Discuss:**                                                                                           \n",
    "\n",
    "‚ÅâÔ∏è  **Q:** Do you notice anything in the training history compared to before? (Look at the noise in the training loss curve) \n",
    "\n",
    "‚ÅâÔ∏è  **Q:** What happens if we remove `model.eval()` before computing validation loss?\n",
    "\n",
    "‚ÅâÔ∏è  **Q:** Run a forward pass on the same input multiple times ‚Äî once with `model.train()` and once with `model.eval()`. What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fjvnxiz5m5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Uncomment to try - answers above question\n",
    "# model5.train()\n",
    "# print(model5(x_val_t[0:1]))\n",
    "# print(model5(x_val_t[0:1]))\n",
    "\n",
    "# model5.eval()\n",
    "# print(model5(x_val_t[0:1]))\n",
    "# print(model5(x_val_t[0:1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89841934",
   "metadata": {
    "id": "89841934"
   },
   "source": [
    "### Data Augmentation with Gaussian Noise\n",
    "\n",
    "Adding random noise to inputs during training is a simple form of data augmentation that acts as a regularizer. Each time the model sees a training example, it sees a slightly perturbed version, which prevents it from memorizing exact input-output mappings. The noise is only applied during training - at evaluation time, the original clean inputs are used. This is especially useful when the dataset is small and collecting more data isn't feasible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MwIcT9C_m8xe",
   "metadata": {
    "id": "MwIcT9C_m8xe"
   },
   "outputs": [],
   "source": [
    "# This class will be utilized later when we add Gaussian noise as a model layer.\n",
    "# 209 students take note!\n",
    "class GaussianNoise(nn.Module):\n",
    "    def __init__(self, stddev):\n",
    "        super().__init__()\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and self.stddev > 0:\n",
    "            noise = torch.randn_like(x) * self.stddev\n",
    "            return x + noise\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ce99a2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 692
    },
    "id": "08ce99a2",
    "outputId": "1fa3ca04-286f-44a7-8f41-ea965c883ad4"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(109)\n",
    "model6 = nn.Sequential(\n",
    "            GaussianNoise(0.1),\n",
    "            nn.Linear(1, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 1)\n",
    "        ).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model6.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epochs = 2000\n",
    "history6 = {'loss': [], 'val_loss': []}\n",
    "for epoch in range(epochs):\n",
    "    model6.train()\n",
    "    optimizer.zero_grad()\n",
    "    preds_train = model6(x_train_t)\n",
    "    loss = criterion(preds_train, y_train_t)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss = loss.item()\n",
    "\n",
    "    model6.eval()\n",
    "    with torch.no_grad():\n",
    "        preds_val = model6(x_val_t)\n",
    "        loss = criterion(preds_val, y_val_t)\n",
    "        val_loss = loss.item()\n",
    "\n",
    "    history6['loss'].append(train_loss)\n",
    "    history6['val_loss'].append(val_loss)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch:4d}: train loss = {train_loss:.4f}, \\\n",
    "              val loss = {val_loss:.4f}\")\n",
    "\n",
    "plot_results(model6, history6, \"Data Augmentation /w Gaussian Noise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b522da",
   "metadata": {},
   "source": [
    "üí¨ **Discuss:**           \n",
    "\n",
    "‚ÅâÔ∏è  **Q:** What does `GaussianNoise(0.1)` do as the first layer of the model? How does it act differently during training vs. evaluation?\n",
    "\n",
    "‚ÅâÔ∏è  **Q:** What happens if the noise standard deviation is increased or decreased? Try different values.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94797d1",
   "metadata": {
    "id": "f94797d1"
   },
   "source": [
    "## Part 2: Classification with Fashion-MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7a1b33",
   "metadata": {
    "id": "4a7a1b33"
   },
   "source": [
    "Having seen regularization in the regression setting, we now apply these ideas to a classification task. We'll build an MLP classifier on Fashion-MNIST, observe overfitting with limited data, and then use data augmentation to improve generalization.\n",
    "\n",
    "---\n",
    "\n",
    "Fashion-MNIST is a dataset of Zalando's article images‚Äîconsisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Fashion-MNIST is intended to serve as a drop-in replacement for MNIST.\n",
    "\n",
    "<img src=\"https://4.bp.blogspot.com/-OQZGt_5WqDo/Wa_Dfa4U15I/AAAAAAAAAUI/veRmAmUUKFA19dVw6XCOV2YLO6n-y_omwCLcBGAs/s400/out.jpg\" width=\"400px\" alt=\"Grid of Fashion-MNIST sample images showing 10 clothing categories: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, and Ankle boot\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06b62fd",
   "metadata": {
    "id": "b06b62fd"
   },
   "source": [
    "### DataLoaders (PyTorch)\n",
    "\n",
    "We use `torchvision.datasets.FashionMNIST` with `transforms.Compose` and PyTorch DataLoaders. We'll take a small train/validation split for faster experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6febd5fb",
   "metadata": {
    "id": "6febd5fb",
    "outputId": "09e4d2f4-b65a-4a6f-c193-7482665d7db6"
   },
   "outputs": [],
   "source": [
    "# Load Fashion-MNIST with normalization\n",
    "mean, std = (0.2860,), (0.3530,)\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "fashion_train = datasets.FashionMNIST(root='data', train=True, download=True, transform=base_transform)\n",
    "fashion_test = datasets.FashionMNIST(root='data', train=False, download=True, transform=base_transform)\n",
    "\n",
    "print(fashion_train, fashion_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37f21d0",
   "metadata": {},
   "source": [
    "üí¨ **Discuss:**    \n",
    "          \n",
    "‚ÅâÔ∏è  **Q:** What does `transforms.Normalize(mean, std)` do? How does it transform the pixel values?    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6b095b",
   "metadata": {
    "id": "5c6b095b"
   },
   "outputs": [],
   "source": [
    "labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "          'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e47d1bb",
   "metadata": {
    "id": "8e47d1bb",
    "outputId": "a2f0ba79-33f6-47a6-d2c5-e41180fcaa61"
   },
   "outputs": [],
   "source": [
    "# Display a few examples (unnormalized for visualization)\n",
    "fig, axs = plt.subplots(3, 5, figsize=(9, 7))\n",
    "for i, ax in enumerate(axs.ravel()):\n",
    "    image, label = fashion_train[i]\n",
    "    # Unnormalize: the transform applied Normalize(mean, std), which computes (x - mean) / std\n",
    "    # To display the original pixel values, we reverse this: x_original = x_normalized * std + mean\n",
    "    image = image * std[0] + mean[0]\n",
    "    ax.imshow(image.squeeze(), cmap='gray')\n",
    "    ax.set_title(labels[label], fontsize=8)\n",
    "    ax.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3f504c",
   "metadata": {
    "id": "da3f504c"
   },
   "source": [
    "We will use PyTorch `Subset` objects to create train/validation/test splits without writing images to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acab26f6",
   "metadata": {
    "id": "acab26f6"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "# Reduce dataset sizes for faster experimentation\n",
    "generator = torch.Generator().manual_seed(109)\n",
    "train_size = 1200\n",
    "val_size = 240\n",
    "test_size = 1000\n",
    "\n",
    "train_val_indices = torch.randperm(len(fashion_train), generator=generator)[:train_size + val_size]\n",
    "train_indices = train_val_indices[:train_size].tolist()\n",
    "val_indices = train_val_indices[train_size:].tolist()\n",
    "test_indices = torch.randperm(len(fashion_test), generator=generator)[:test_size].tolist()\n",
    "\n",
    "train_subset = Subset(fashion_train, train_indices)\n",
    "val_subset = Subset(fashion_train, val_indices)\n",
    "test_subset = Subset(fashion_test, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cedea5e",
   "metadata": {
    "id": "7cedea5e",
    "outputId": "1d7b4de1-700f-400a-cca5-fe7756b35841"
   },
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "data_batch, labels_batch = next(iter(train_loader))\n",
    "print('data batch shape:', data_batch.shape)\n",
    "print('labels batch shape:', labels_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3492a4b",
   "metadata": {},
   "source": [
    "üí¨ **Discuss:**    \n",
    "\n",
    "‚ÅâÔ∏è  **Q:** What is a DataLoader? Why do we use it instead of passing the full dataset to the model at once?   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce59b803",
   "metadata": {
    "id": "ce59b803"
   },
   "source": [
    "> **‚ùì Question 6: Subset Splits**\n",
    ">\n",
    "> 1. Why do we create separate train/validation/test subsets instead of reusing the full dataset?\n",
    "> 2. How could class imbalance in a small subset affect validation accuracy?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a628b063",
   "metadata": {
    "id": "a628b063"
   },
   "source": [
    "### Build a Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a204849b",
   "metadata": {
    "id": "a204849b"
   },
   "outputs": [],
   "source": [
    "class FashionMLP(nn.Module):\n",
    "    def __init__(self, input_dim=28*28, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tkyheg4kkrp",
   "metadata": {
    "id": "tkyheg4kkrp"
   },
   "outputs": [],
   "source": [
    "def train_classifier(model, train_loader, val_loader, optimizer, criterion, epochs=30, patience=None):\n",
    "    history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}\n",
    "\n",
    "    # Early stopping state\n",
    "    best_val_loss = np.inf\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(xb)\n",
    "            loss = criterion(outputs, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * xb.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_acc = correct / total\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                outputs = model(xb)\n",
    "                loss = criterion(outputs, yb)\n",
    "                val_loss += loss.item() * xb.size(0)\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                val_correct += (preds == yb).sum().item()\n",
    "                val_total += yb.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        history['loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['accuracy'].append(train_acc)\n",
    "        history['val_accuracy'].append(val_acc)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if patience is not None:\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                best_model_state = copy.deepcopy(model.state_dict())\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "\n",
    "    # Restore best model weights\n",
    "    if patience is not None and best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_classifier(model, loader, criterion):\n",
    "    model.eval()\n",
    "    loss_total = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            outputs = model(xb)\n",
    "            loss = criterion(outputs, yb)\n",
    "            loss_total += loss.item() * xb.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == yb).sum().item()\n",
    "            total += yb.size(0)\n",
    "    return loss_total / len(loader.dataset), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a90b135",
   "metadata": {
    "id": "8a90b135"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(109)\n",
    "model_cls = FashionMLP().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_cls.parameters(), lr=0.001)\n",
    "\n",
    "history_cls = train_classifier(\n",
    "    model_cls,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    epochs=30,\n",
    "    patience=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdad8d42",
   "metadata": {
    "id": "fdad8d42",
    "outputId": "53cd444f-2c41-49c8-837d-fca7b28c30b9"
   },
   "outputs": [],
   "source": [
    "train_loss, train_acc = evaluate_classifier(model_cls, train_loader, criterion)\n",
    "val_loss, val_acc = evaluate_classifier(model_cls, val_loader, criterion)\n",
    "test_loss, test_acc = evaluate_classifier(model_cls, test_loader, criterion)\n",
    "\n",
    "print(f\"Train ‚Äî Loss: {train_loss:.4f}, Acc: {train_acc:.2f}\")\n",
    "print(f\"Val   ‚Äî Loss: {val_loss:.4f}, Acc: {val_acc:.2f}\")\n",
    "print(f\"Test  ‚Äî Loss: {test_loss:.4f}, Acc: {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb8a48b",
   "metadata": {
    "id": "7eb8a48b",
    "outputId": "18e775c9-788d-4d74-d1b8-bbead6106675"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "print(f\"Val Acc at last training epoch (before best-model restore): {history_cls['val_accuracy'][-1]:.2f}\")\n",
    "axs[0].plot(history_cls['accuracy'])\n",
    "axs[0].plot(history_cls['val_accuracy'])\n",
    "axs[0].set_title('Baseline Classifier ‚Äî Accuracy')\n",
    "axs[0].set_ylabel('accuracy')\n",
    "axs[0].set_xlabel('epoch')\n",
    "axs[0].grid(True, alpha=0.3)\n",
    "axs[0].legend(['train', 'validation'], loc='upper left')\n",
    "\n",
    "axs[1].plot(history_cls['loss'])\n",
    "axs[1].plot(history_cls['val_loss'])\n",
    "axs[1].set_title('Baseline Classifier ‚Äî Loss')\n",
    "axs[1].set_ylabel('loss')\n",
    "axs[1].set_xlabel('epoch')\n",
    "axs[1].grid(True, alpha=0.3)\n",
    "axs[1].legend(['train', 'validation'], loc='upper left')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c6085d",
   "metadata": {
    "id": "b8c6085d"
   },
   "source": [
    "### Data Augmentation\n",
    "\n",
    "In Part 1, we added Gaussian noise directly inside the model as a regularizer. For image data, we can go further ‚Äî applying transformations that exploit known invariances. A T-shirt is still a T-shirt whether flipped horizontally or slightly rotated. By applying random transforms on-the-fly, the model sees a different variation of each image every epoch, even though the dataset itself remains 1,200 samples. Below we combine random flips, small rotations, and Gaussian noise as a `torchvision` transform pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7fafac",
   "metadata": {
    "id": "6b7fafac"
   },
   "outputs": [],
   "source": [
    "class AddGaussianNoise:\n",
    "    def __init__(self, std=0.05):\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        return torch.clamp(tensor + torch.randn_like(tensor) * self.std, 0.0, 1.0)\n",
    "\n",
    "augment_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  #Default probability is 0.5\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    AddGaussianNoise(0.05),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "fashion_train_aug = datasets.FashionMNIST(root='data', train=True, download=False, transform=augment_transform)\n",
    "train_subset_aug = Subset(fashion_train_aug, train_indices)\n",
    "train_loader_aug = torch.utils.data.DataLoader(train_subset_aug, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w547xxqmde",
   "metadata": {
    "id": "w547xxqmde"
   },
   "source": [
    "> **‚ùì Question 7: Choosing Augmentations**\n",
    ">\n",
    "> Not all augmentations are appropriate for every dataset. Consider the transforms we used: `RandomHorizontalFlip`, `RandomRotation(10)`, and `AddGaussianNoise`.\n",
    ">\n",
    "> 1. Why is `RandomHorizontalFlip` a reasonable augmentation for Fashion-MNIST? For which classes (if any) might it *not* preserve the label?\n",
    "> 2. Why do we limit rotation to just 10 degrees? What could go wrong with larger rotations (e.g., 90¬∞)?\n",
    "> 3. We did **not** include `RandomVerticalFlip`. Why would vertical flips be a bad augmentation for clothing images?\n",
    "> 4. If you were working with a medical imaging dataset (e.g., chest X-rays), which of these augmentations would still be appropriate and which would not?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406cb7c1",
   "metadata": {
    "id": "406cb7c1"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(109)\n",
    "model_cls_aug = FashionMLP().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_cls_aug.parameters(), lr=0.001)\n",
    "\n",
    "history_aug = train_classifier(\n",
    "    model_cls_aug,\n",
    "    train_loader_aug,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    epochs=60,\n",
    "    patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a179bd0",
   "metadata": {
    "id": "2a179bd0",
    "outputId": "dadb3172-d92f-4eb5-baec-44b108820fe6"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "print(f\"Final epoch val Acc: {history_aug['val_accuracy'][-1]:.2f}\")\n",
    "print(f\"Best epoch val Acc: {max(history_aug['val_accuracy']):.2f}\")\n",
    "\n",
    "axs[0].plot(history_aug['accuracy'])\n",
    "axs[0].plot(history_aug['val_accuracy'])\n",
    "axs[0].set_title('Augmented Classifier ‚Äî Accuracy')\n",
    "axs[0].set_ylabel('accuracy')\n",
    "axs[0].set_xlabel('epoch')\n",
    "axs[0].legend(['train', 'validation'], loc='upper left')\n",
    "\n",
    "axs[1].plot(history_aug['loss'])\n",
    "axs[1].plot(history_aug['val_loss'])\n",
    "axs[1].set_title('Augmented Classifier ‚Äî Loss')\n",
    "axs[1].set_ylabel('loss')\n",
    "axs[1].set_xlabel('epoch')\n",
    "axs[1].legend(['train', 'validation'], loc='upper left')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6cb9ec",
   "metadata": {
    "id": "df6cb9ec",
    "outputId": "4f92b417-82ee-4f56-b942-5700b6c873b8"
   },
   "outputs": [],
   "source": [
    "train_loss, train_acc = evaluate_classifier(model_cls_aug, train_loader, criterion)\n",
    "val_loss, val_acc = evaluate_classifier(model_cls_aug, val_loader, criterion)\n",
    "test_loss, test_acc = evaluate_classifier(model_cls_aug, test_loader, criterion)\n",
    "\n",
    "print(f\"Train ‚Äî Loss: {train_loss:.4f}, Acc: {train_acc:.2f}\")\n",
    "print(f\"Val   ‚Äî Loss: {val_loss:.4f}, Acc: {val_acc:.2f}\")\n",
    "print(f\"Test  ‚Äî Loss: {test_loss:.4f}, Acc: {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3l0pylapqvk",
   "metadata": {
    "id": "3l0pylapqvk"
   },
   "source": [
    "**Results comparison:** Augmentation improved validation accuracy over the baseline on our small 1,200-sample training set, while test accuracy remained similar. With only 1,000 test samples, a few points of difference can be noise, and the validation split comes from the same `fashion_train` pool, so gains may not transfer to the separate test set. Always rely on a truly held-out test set and avoid over-interpreting small differences.\n",
    "\n",
    "Note: exact metrics can vary across runs and hardware (CPU vs GPU) due to nondeterminism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ekruz59e9t",
   "metadata": {
    "id": "4ekruz59e9t"
   },
   "source": [
    "> **‚ùì Question 8: Model Layer vs. Data Transform for Noise**\n",
    ">\n",
    "> In Part 1 we added Gaussian noise as a **model layer** (`GaussianNoise(nn.Module)`), while in Part 2 we added it as a **data transform** (`AddGaussianNoise` in `transforms.Compose`). Both inject random noise during training to regularize the model.\n",
    ">\n",
    "> 1. At what point in the pipeline does noise get applied in each approach? How does this affect what the noise \"means\" (pixel space vs. normalized space)?\n",
    "> 2. What are the trade-offs of each approach? When might you prefer one over the other?\n",
    "> 3. The `AddGaussianNoise` transform clamps values to [0, 1]. Why is this necessary here but not in the `GaussianNoise` module?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vXPR-Adp5xsx",
   "metadata": {
    "id": "vXPR-Adp5xsx"
   },
   "source": [
    "## A Note on GPU Device Management\n",
    "\n",
    "This notebook is configured to use a GPU when available (`device = 'cuda'`), falling back to CPU otherwise. Working with GPUs in PyTorch requires explicitly moving data and models to the same device. Here's the pattern you'll see throughout:\n",
    "\n",
    "**1. Model to device** ‚Äî `model.to(device)` moves all model parameters (weights, biases, BatchNorm statistics) to the GPU. This only needs to be done once after creating the model.\n",
    "\n",
    "**2. Batches to device** ‚Äî Inside the training loop, each mini-batch must also be moved: `xb = xb.to(device)` and `yb = yb.to(device)`. DataLoaders return CPU tensors by default, so this transfer happens every iteration. If the model is on GPU but the data is on CPU (or vice versa), PyTorch will raise a runtime error.\n",
    "\n",
    "**3. Getting results back for evaluation** ‚Äî There are two separate concerns when extracting values from PyTorch tensors:\n",
    "\n",
    "- **Computation graph**: Tensors produced during a forward pass track operations for automatic differentiation (regardless of whether they're on CPU or GPU). To disconnect from this graph, either wrap code in `torch.no_grad()` (prevents tracking entirely ‚Äî used in eval loops) or call `.detach()` on individual tensors.\n",
    "\n",
    "- **Device transfer**: GPU tensors can't be converted to NumPy arrays directly ‚Äî they must first move to CPU via `.cpu()`.\n",
    "\n",
    "In practice you'll see two patterns in this notebook:\n",
    "- **Scalars** (losses, accuracy counts): use `.item()` to extract a Python number ‚Äî this handles both detaching and device transfer in one call. Example from the training loop: `loss.item()`, `(preds == yb).sum().item()`.\n",
    "- **Full tensors** (predictions for plotting): use `.detach().cpu().numpy()` to get a NumPy array. Example from `plot_predictions`: `preds = model(x_tensor).detach().cpu().numpy()`. Inside a `torch.no_grad()` block the `.detach()` is technically redundant, but it's common practice to include it for safety.\n",
    "\n",
    "In short: **model and data must live on the same device**, and **results must come back to CPU** before you can use them with non-PyTorch libraries like NumPy or matplotlib."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
